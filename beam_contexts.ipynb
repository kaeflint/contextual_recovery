{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from functools import partial\n",
    "import nltk\n",
    "from src.contextual_bart import ContextualisedBartModel,BartForContextualRecovery,SimplifiedBeamSearch\n",
    "from src.dataset_processor import load_all_data\n",
    "from src.utils import SmartCollator, get_args, setuptokenizer\n",
    "from src.dataset_processor import (\n",
    "    ContextGenerationDataset,\n",
    ")\n",
    "from transformers import BartTokenizer, BartConfig,BartForConditionalGeneration\n",
    "from src.model_utils import CustomTrainer, get_training_arguments\n",
    "import torch\n",
    "from src.config import DATASET_PATH\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "import pickle as pk\n",
    "import torch\n",
    "from transformers import (    AutoTokenizer,\n",
    "          AutoModelForSeq2SeqLM,\n",
    "         LogitsProcessorList,    MinLengthLogitsProcessor, StoppingCriteriaList, MaxLengthCriteria,\n",
    "         TopKLogitsWarper, TemperatureLogitsWarper,BeamSearchScorer,)\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "DATASET_PATH = \"summarisation_data/\"\n",
    "\n",
    "def generate_tokenizer_and_data(args):\n",
    "\n",
    "    # load the dataset\n",
    "\n",
    "    train_data_packet = load_all_data(DATASET_PATH, mode=\"train\")\n",
    "    dev_data_packet = load_all_data(DATASET_PATH, mode=\"dev\")\n",
    "    test_data_packet = load_all_data(DATASET_PATH,mode=\"test\")\n",
    "\n",
    "    print(f\"Training Data size: {len(train_data_packet)}\")\n",
    "    print(f\"Training Data size: {len(test_data_packet)}\")\n",
    "\n",
    "    model_base = args.model_base\n",
    "    tokenizer = setuptokenizer(\n",
    "        model_base=model_base,\n",
    "        special_tokens=[],\n",
    "    )\n",
    "    tokenizer.add_tokens([args.sep_token])\n",
    "\n",
    "    train_dataset = ContextGenerationDataset(\n",
    "        tokenizer=tokenizer, nb_records=len(train_data_packet), max_len=720,\n",
    "        context_seperator=args.sep_token,\n",
    "        is_auto_encoder_data=not args.is_not_auto_encoder_data,\n",
    "        use_special_token=True,\n",
    "    )\n",
    "    train_dataset.change_data_mode(1)\n",
    "    train_dataset.set_record(train_data_packet)\n",
    "\n",
    "    test_dataset = ContextGenerationDataset(\n",
    "        tokenizer=tokenizer, nb_records=len(test_data_packet), \n",
    "        max_len=700,\n",
    "        context_seperator=args.sep_token,\n",
    "        is_auto_encoder_data=not args.is_not_auto_encoder_data,\n",
    "    )\n",
    "    test_dataset.change_data_mode(1)\n",
    "    test_dataset.set_record(test_data_packet)\n",
    "    \n",
    "    dev_dataset = ContextGenerationDataset(\n",
    "        tokenizer=tokenizer, nb_records=len(dev_data_packet), \n",
    "        max_len=700,\n",
    "        context_seperator=args.sep_token,\n",
    "        is_auto_encoder_data=not args.is_not_auto_encoder_data,\n",
    "    )\n",
    "    test_dataset.change_data_mode(1)\n",
    "    test_dataset.set_record(test_data_packet)\n",
    "\n",
    "    return train_dataset, dev_dataset,test_dataset, [train_data_packet,dev_data_packet,test_data_packet]\n",
    "\n",
    "\n",
    "\n",
    "def model_init(\n",
    "    vocab_size,\n",
    "    context_delimiter_id,\n",
    "    model_base=\"facebook/bart-base\",\n",
    "    use_random_restriction=False,\n",
    "    section_prob=(0.25, 0.45),\n",
    "    device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n",
    "):\n",
    "    def build_model():\n",
    "        bart_config = BartConfig.from_pretrained(model_base)\n",
    "        bart_config.context_delimiter_id = context_delimiter_id\n",
    "        bart_config.use_random_restriction = use_random_restriction\n",
    "        bart_config.section_prob = section_prob\n",
    "\n",
    "        generator = BartForContextualRecovery.from_pretrained(\n",
    "            model_base, config=bart_config, ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "        # update the tokens\n",
    "        generator.resize_token_embeddings(vocab_size)  # type: ignore\n",
    "        return generator.to(device)  # type: ignore\n",
    "    return build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing files:  ['summarisation_data/xsum_train.csv']\n",
      "processing files:  ['summarisation_data/xsum_dev.csv']\n",
      "processing files:  ['summarisation_data/xsum_test.csv']\n",
      "Training Data size: 162548\n",
      "Training Data size: 9049\n",
      "The model will be trained as a non auto-encoder\n",
      "The model will be trained as a non auto-encoder\n",
      "The model will be trained as a non auto-encoder\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Args:\n",
    "    model_base: str\n",
    "    sep_token: str = \"[SEP]\"\n",
    "    is_not_auto_encoder_data: bool = True\n",
    "    \n",
    "    \n",
    "args = Args(model_base=\"facebook/bart-base\")\n",
    "train_dataset, dev_dataset,test_dataset, [train_data_packet,dev_data_packet,test_data_packet] = generate_tokenizer_and_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mores= []\n",
    "for idx,da in enumerate(train_dataset):\n",
    "    if da.input_ids.shape[0] > 600:\n",
    "        mores.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmores= []\n",
    "for idx,da in enumerate(test_dataset):\n",
    "    if da.input_ids.shape[0] > 512:\n",
    "        cmores.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cmores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter_points=train_dataset[498269].input_ids==train_dataset._context_delimiter_id\n",
    "delimiter_points_idx = delimiter_points.nonzero(as_tuple=True)[-1][0]\n",
    "delimiter_points_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[67].labels.shape,train_dataset[67].input_ids.shape,train_dataset[67].section_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[67].input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_delimiter_id = train_dataset.tokenizer.get_vocab()['[SEP]']\n",
    "\n",
    "train_model_path = \"trained_models_sum/bart_base_model_full/checkpoint-81275/pytorch_model.bin\"\n",
    "#\"trained_models_mtl/bart_base_model_full/checkpoint-263195/pytorch_model.bin\"\n",
    "\n",
    "generator = model_init(len(train_dataset.tokenizer),\n",
    "                       context_delimiter_id=context_delimiter_id,\n",
    "                       model_base=args.model_base,use_random_restriction=False)()\n",
    "\n",
    "state_dict = torch.load(train_model_path)\n",
    "generator.load_state_dict(state_dict)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.contextual_bart import EncoderOutputs\n",
    "class SimplifiedBeamSearch:\n",
    "    def __init__(self, generator, tokenizer) -> None:\n",
    "        self.generator = generator\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        shrink_encoder_output = 1,\n",
    "        num_beams=5,\n",
    "        min_length=100,\n",
    "        max_length=500,\n",
    "        top_k=50,\n",
    "        temperature=0.85,\n",
    "    ):\n",
    "\n",
    "        # initialise decoder input_ids\n",
    "        decoder_input_ids = torch.ones(\n",
    "            (num_beams, 1), device=self.generator.device, dtype=torch.long\n",
    "        )\n",
    "        decoder_input_ids = (\n",
    "            decoder_input_ids * self.generator.config.decoder_start_token_id\n",
    "        )\n",
    "        \n",
    "        encoder = self.generator.get_encoder()\n",
    "        \n",
    "         \n",
    "        \n",
    "        encoder_outputs = self.generator.get_encoder()(\n",
    "                input_ids.repeat_interleave(num_beams, dim=0),\n",
    "                attention_mask.repeat_interleave(num_beams, dim=0),\n",
    "                return_dict=True,\n",
    "            )\n",
    "        \n",
    "        enc_num_tokens = encoder_outputs[0].shape[1]\n",
    "        \n",
    "        #print(encoder_outputs[0].shape)\n",
    "        attention_mask_ = encoder_outputs.cleaned_mask\n",
    "        if shrink_encoder_output< 1:\n",
    "            num_usable_tokens = round(shrink_encoder_output*enc_num_tokens)\n",
    "            \n",
    "            encoder_outputs = EncoderOutputs(\n",
    "                last_hidden_state=encoder_outputs[0][:,num_usable_tokens:,:],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "                cleaned_mask=encoder_outputs[3] if len(encoder_outputs) > 3 else None,\n",
    "            )\n",
    "        \n",
    "        print(f\"Generating text using {encoder_outputs[0].shape[1]} token embeddings out off {input_ids.shape[-1]}\")\n",
    "            \n",
    "        \n",
    "        print(\"Encoder output shape: \",encoder_outputs[0].shape)\n",
    "        \n",
    "        print(\"Attention mask: \",attention_mask_.shape)\n",
    "\n",
    "        model_kwargs = {\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"attention_mask\": attention_mask_,\n",
    "        }\n",
    "        beam_scorer = BeamSearchScorer(\n",
    "            batch_size=attention_mask.shape[0],\n",
    "            num_beams=num_beams,\n",
    "            device=self.generator.device,\n",
    "        )\n",
    "\n",
    "        logits_processor = LogitsProcessorList(\n",
    "            [\n",
    "                MinLengthLogitsProcessor(\n",
    "                    1, eos_token_id=self.generator.config.eos_token_id\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        logits_warper = LogitsProcessorList(\n",
    "            [\n",
    "                TopKLogitsWarper(top_k),\n",
    "                TemperatureLogitsWarper(temperature),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        outputs = self.generator.beam_sample(\n",
    "            decoder_input_ids,\n",
    "            beam_scorer,\n",
    "            max_length=max_length,\n",
    "            logits_processor=logits_processor,\n",
    "            # stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length)),\n",
    "            logits_warper=logits_warper,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "\n",
    "        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be trained as a non auto-encoder\n"
     ]
    }
   ],
   "source": [
    "dataset = ContextGenerationDataset(test_dataset.tokenizer,\n",
    "                                   nb_records=1,\n",
    "                                   section_boundary=(0.4,0.48),\n",
    "                                   \n",
    "        context_seperator=args.sep_token,\n",
    "        is_auto_encoder_data=not args.is_not_auto_encoder_data,\n",
    "                                   use_random_restrictive=True)\n",
    "dataset.change_data_mode(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, torch.Size([1, 256]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataset_processor import ContextualGenerationData\n",
    "from pytorch_lightning import seed_everything\n",
    "data = ContextualGenerationData(input=\"\"\"\n",
    "                                We are helping the community work together towards the goal of advancing Machine Learning 🔥.\n",
    "The Hugging Face Hub is a platform with over 60K models, 6K datasets, and 6K demos in which people can easily collaborate in their ML workflows. \n",
    "The Hub works as a central place where anyone can share, explore, discover, and experiment with open-source Machine Learning.\n",
    " No single company, including the Tech Titans, will be able to “solve AI” by themselves - the only way we'll achieve this is by sharing knowledge and resources in a community-centric approach. We are building the largest open-source collection of models, datasets, demos and metrics on the Hugging Face Hub to democratize and advance ML for everyone 🚀.\n",
    "                                \"\"\".replace(\"\\n\",\"\").strip(),output=\"\")\n",
    "kk= 45\n",
    "batch = test_dataset[kk]#dataset.procesTexts(data)\n",
    "b_input_ids = batch.input_ids.view(1, -1).to(device)\n",
    "b_input_mask = batch.attention_mask.view(1, -1).to(device)\n",
    "batch.section_point, b_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.tokenizer.batch_decode(b_input_ids[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scottish legal authorities have granted permission for Twitter to be used to report the conclusion of a murder trial at the High Court.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_packet[kk].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The sentencing of a man convicted of murdering a bookkeeper in Argyll will be streamed live on Twitter, the Lord Chief Justice has confirmed.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb=generator.generate(input_ids=b_input_ids,\n",
    "            attention_mask=b_input_mask,\n",
    "            num_beams=10,\n",
    "            do_sample=True,\n",
    "            max_new_tokens=300)\n",
    "test_dataset.tokenizer.batch_decode(bb,clean_up_tokenization_spaces=True,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(100)\n",
    "bb= SimplifiedBeamSearch(generator,dataset.tokenizer)\n",
    "bb.generate(input_ids=b_input_ids,\n",
    "            attention_mask=b_input_mask,\n",
    "            shrink_encoder_output=1,\n",
    "            num_beams=2,\n",
    "            max_length=370,\n",
    "            temperature=0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.1*25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_embedding_sections(batch_size, max_length, low=0.45, high=0.55):\n",
    "    deletion_section_probs = np.random.uniform(size=(batch_size,), low=low, high=high)\n",
    "    deletion_section = max_length * deletion_section_probs\n",
    "    return torch.round(\n",
    "        torch.FloatTensor(deletion_section),\n",
    "    ).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_random_embedding_sections(15,200,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from logging import Logger\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    ")\n",
    "from transformers.models.bart.modeling_bart import (\n",
    "    BartConfig,\n",
    "    BartDecoder,\n",
    "    BartEncoderLayer,\n",
    "    BartLearnedPositionalEmbedding,\n",
    "    BartPretrainedModel,\n",
    "    BaseModelOutput,\n",
    "    CrossEntropyLoss,\n",
    "    Seq2SeqLMOutput,\n",
    "    Seq2SeqModelOutput,\n",
    "    _expand_mask,\n",
    "    shift_tokens_right,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EncoderOutputs(BaseModelOutput):\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attention_mask: torch.LongTensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestrictedBartEncoder(BartPretrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
    "    [`BartEncoderLayer`].\n",
    "\n",
    "    Args:\n",
    "        config: BartConfig\n",
    "        embed_tokens (nn.Embedding): output embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "        self.layerdrop = config.encoder_layerdrop\n",
    "\n",
    "        embed_dim = config.d_model\n",
    "        self.padding_idx = config.pad_token_id\n",
    "\n",
    "        #self._context_delimiter_id = config.context_delimiter_id\n",
    "        self._min_section_prob,self._max_section_prob = config.section_prob\n",
    "        self.max_source_positions = config.max_position_embeddings\n",
    "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
    "\n",
    "        if embed_tokens is not None:\n",
    "            self.embed_tokens = embed_tokens\n",
    "        else:\n",
    "            self.embed_tokens = nn.Embedding(\n",
    "                config.vocab_size, embed_dim, self.padding_idx\n",
    "            )\n",
    "\n",
    "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
    "            config.max_position_embeddings,\n",
    "            embed_dim,\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [BartEncoderLayer(config) for _ in range(config.encoder_layers)]\n",
    "        )\n",
    "        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "        \n",
    "    \n",
    "    def _get_random_embedding_sections(self,batch_size, max_length, low=0.40, high=0.6):\n",
    "        deletion_section_probs = np.random.uniform(size=(batch_size,), low=low, high=high)\n",
    "        deletion_section = max_length * deletion_section_probs\n",
    "        return torch.round(torch.FloatTensor(deletion_section),).long()\n",
    "    def _strip_context(self, input_ids, embeddings, attention_mask):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_ids:\n",
    "        :param embeddings:\n",
    "        :param attention_mask:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # identify the locations of the context_delimiter in each of the input sequence\n",
    "        if type(input_ids) is list:\n",
    "            input_ids = torch.LongTensor(\n",
    "                input_ids,\n",
    "            )\n",
    "            \n",
    "        # Get the batch-size and the max_len of embeddings\n",
    "        batch_size, batch_max_length,_ =  embeddings.shape\n",
    "        \n",
    "        #delimiter_points.nonzero(as_tuple=True)[-1]\n",
    "        \n",
    "        # Randomly select parts of the encoder output to \n",
    "        delimiter_points_idxs = self._get_random_embedding_sections(batch_size,\n",
    "                                                                    batch_max_length,\n",
    "                                                                    self._min_section_prob,\n",
    "                                                                    self._max_section_prob)\n",
    "\n",
    "        all_embeddings = []\n",
    "        all_attention_masks = []\n",
    "        all_input_ids = []\n",
    "        max_length = 0\n",
    "        embedding_dim = embeddings.shape[-1]\n",
    "\n",
    "        # For item in input_ids, embeddings, attention_mask, input_ids, select the\n",
    "        # portion of the tensor after the delimiter_point_id\n",
    "        for delimiter_point_id, embedding, att_mask in zip(\n",
    "            delimiter_points_idxs, embeddings, attention_mask\n",
    "        ):\n",
    "            embedding = embedding[delimiter_point_id + 1 :, :]\n",
    "            if max_length < embedding.shape[0]:\n",
    "                max_length = embedding.shape[0]\n",
    "            all_embeddings.append(embedding)\n",
    "            all_attention_masks.append(att_mask[delimiter_point_id + 1 :])\n",
    "\n",
    "        # Reshape all the section of interest for each item in all_input_ids, all_embeddings, all_attention_masks to\n",
    "        # the same size\n",
    "        batch_embeddings: List = list()\n",
    "        batch_attention_masks: List = list()\n",
    "\n",
    "        for idx, (embedding, att_mask) in enumerate(\n",
    "            zip(all_embeddings, all_attention_masks)\n",
    "        ):\n",
    "            len_diff = max_length - embedding.shape[0]\n",
    "            if max_length > embedding.shape[0]:\n",
    "                pad_tensor = torch.zeros(len_diff, embedding_dim).to(embedding.device)\n",
    "                embedding = torch.concat([embedding, pad_tensor], dim=0)\n",
    "\n",
    "                attn_pads = torch.zeros(\n",
    "                    len_diff,\n",
    "                ).to(att_mask.device)\n",
    "                att_mask = torch.concat([att_mask, attn_pads], -1)\n",
    "\n",
    "            batch_embeddings += [embedding.view(-1, max_length, embedding_dim)]\n",
    "            batch_attention_masks += [att_mask.view(-1, max_length)]\n",
    "        \n",
    "        # Create the final tensors with the contexts removed\n",
    "        batch_attention_masks = torch.concat(batch_attention_masks, 0)\n",
    "        batch_embeddings = torch.concat(batch_embeddings, 0)\n",
    "        return batch_embeddings, batch_attention_masks\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
    "                provide it.\n",
    "\n",
    "                Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "                [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "                [What are input IDs?](../glossary#input-ids)\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time\"\n",
    "            )\n",
    "        elif input_ids is not None:\n",
    "            input = input_ids\n",
    "            input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "        elif inputs_embeds is not None:\n",
    "            input = inputs_embeds[:, :, -1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
    "\n",
    "        embed_pos = self.embed_positions(input)\n",
    "\n",
    "        hidden_states = inputs_embeds + embed_pos\n",
    "        hidden_states = self.layernorm_embedding(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(\n",
    "            hidden_states, p=self.dropout, training=self.training\n",
    "        )\n",
    "\n",
    "        attention_mask_ = attention_mask\n",
    "\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask_ = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        # check if head_mask has a correct number of layers specified if desired\n",
    "        if head_mask is not None:\n",
    "            if head_mask.size()[0] != (len(self.layers)):\n",
    "                raise ValueError(\n",
    "                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                    f\" {head_mask.size()[0]}.\"\n",
    "                )\n",
    "\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (\n",
    "                dropout_probability < self.layerdrop\n",
    "            ):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(encoder_layer),\n",
    "                        hidden_states,\n",
    "                        attention_mask_,\n",
    "                        (head_mask[idx] if head_mask is not None else None),\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = encoder_layer(\n",
    "                        hidden_states,\n",
    "                        attention_mask_,\n",
    "                        layer_head_mask=(\n",
    "                            head_mask[idx] if head_mask is not None else None\n",
    "                        ),\n",
    "                        output_attentions=output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        hidden_states, batch_encoder_attention_masks = self._strip_context(\n",
    "            input_ids, hidden_states, attention_mask\n",
    "        )\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    encoder_states,\n",
    "                    all_attentions,\n",
    "                    batch_encoder_attention_masks,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "\n",
    "        return EncoderOutputs(\n",
    "            last_hidden_state=hidden_states,\n",
    "            hidden_states=encoder_states,\n",
    "            attentions=all_attentions,\n",
    "            attention_mask=batch_encoder_attention_masks,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "config = copy.deepcopy(generator.config)\n",
    "config.section_prob = (0.2,0.65)\n",
    "config.context_delimiter_id = generator.model.get_encoder()._context_delimiter_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.model.get_encoder()._context_delimiter_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restrictive_encoder = RestrictedBartEncoder.from_pretrained(\"facebook/bart-base\",config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouut = restrictive_encoder(b_input_ids.repeat_interleave(4, dim=0),b_input_mask.repeat_interleave(4, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"race\",'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc=dataset['train'].features[\"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][1]['article'].replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "400*0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from src.utils import setuptokenizer\n",
    "bart_tokenizer = setuptokenizer('facebook/bart-base',special_tokens=[\"[SEP]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.rand((2,128,300))\n",
    "attention_mask = torch.ones((2,128))\n",
    "attention_mask[0,90:]= 0\n",
    "attention_mask[1,110:]= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 90, 110])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attention_mask==1).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = (\n",
    "        model_output  # First element of model_output contains all token embeddings\n",
    "    )\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink_embeddings(embeddings,\n",
    "                      attention_mask, \n",
    "                      percentage=0):\n",
    "\n",
    "    if percentage == 0:\n",
    "        return embeddings, attention_mask\n",
    "    seq_lengths = (attention_mask == 1).sum(dim=1)\n",
    "    \n",
    "    embedding_dim = embeddings.shape[-1]\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_attention_masks = []\n",
    "    max_length = 0\n",
    "    for sl, embed, attn in zip(seq_lengths, embeddings, attention_mask):\n",
    "        summary_point =  torch.round(percentage * sl).long()\n",
    "        print(summary_point)\n",
    "        summary_embedding = embed[:summary_point, :]\n",
    "        summary_attention = attn[:summary_point]\n",
    "        summary_embedding = mean_pooling(summary_embedding.view(1,-1,embedding_dim), summary_attention)\n",
    "\n",
    "        new_embedding = torch.concat([\n",
    "            summary_embedding.view(1, -1).to(embed.device),\n",
    "            embed[summary_point:, :]],\n",
    "            dim=0,\n",
    "        )\n",
    "        new_attention = torch.concat([\n",
    "            torch.ones((1,)).to(embed.device), attn[summary_point:, ]], -1\n",
    "        )\n",
    "\n",
    "        all_embeddings.append(new_embedding)\n",
    "        all_attention_masks.append(new_attention)\n",
    "\n",
    "        if max_length < new_embedding.shape[0]:\n",
    "            max_length = new_embedding.shape[0]\n",
    "\n",
    "    # Reshape all the section of interest for each item in all_input_ids, all_embeddings, all_attention_masks to\n",
    "    # the same size\n",
    "    batch_embeddings: List = list()\n",
    "    batch_attention_masks: List = list()\n",
    "\n",
    "    for idx, (embedding, att_mask) in enumerate(\n",
    "        zip(all_embeddings, all_attention_masks)\n",
    "    ):\n",
    "        len_diff = max_length - embedding.shape[0]\n",
    "        embedding_dim = embedding.shape[1]\n",
    "        if max_length > embedding.shape[0]:\n",
    "            pad_tensor = torch.zeros(len_diff, embedding_dim).to(embedding.device)\n",
    "            embedding = torch.concat([embedding, pad_tensor], dim=0)\n",
    "\n",
    "            attn_pads = torch.zeros(\n",
    "                len_diff,\n",
    "            ).to(att_mask.device)\n",
    "            att_mask = torch.concat([att_mask, attn_pads], -1)\n",
    "\n",
    "        batch_embeddings += [embedding.view(-1, max_length, embedding_dim)]\n",
    "        batch_attention_masks += [att_mask.view(-1, max_length)]\n",
    "\n",
    "    # Create the final tensors with the contexts removed\n",
    "    batch_attention_masks = torch.concat(batch_attention_masks, 0)\n",
    "    batch_embeddings = torch.concat(batch_embeddings, 0)\n",
    "    \n",
    "    return batch_embeddings,batch_attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(1)\n",
      "torch.Size([2, 40, 300])\n"
     ]
    }
   ],
   "source": [
    "embeddings,attention_mask = shrink_embeddings(embeddings,attention_mask,0.65)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 40, 300])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 300])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 115, 300])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bart.modeling_bart import (\n",
    "    BartConfig,\n",
    "    BartDecoder,\n",
    "    BartEncoderLayer,\n",
    "    BartLearnedPositionalEmbedding,\n",
    "    BartPretrainedModel,\n",
    "    BaseModelOutput,\n",
    "    CrossEntropyLoss,\n",
    "    Seq2SeqLMOutput,\n",
    "    Seq2SeqModelOutput,\n",
    "    _expand_mask,\n",
    "    shift_tokens_right,\n",
    ")\n",
    "from src.model_utils import EncoderOutputs\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartEncoder(BartPretrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
    "    [`BartEncoderLayer`].\n",
    "\n",
    "    Args:\n",
    "        config: BartConfig\n",
    "        embed_tokens (nn.Embedding): output embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "        self.layerdrop = config.encoder_layerdrop\n",
    "\n",
    "        embed_dim = config.d_model\n",
    "        self.padding_idx = config.pad_token_id\n",
    "\n",
    "        self._context_delimiter_id = config.context_delimiter_id\n",
    "        self.max_source_positions = config.max_position_embeddings\n",
    "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
    "\n",
    "        if embed_tokens is not None:\n",
    "            self.embed_tokens = embed_tokens\n",
    "        else:\n",
    "            self.embed_tokens = nn.Embedding(\n",
    "                config.vocab_size, embed_dim, self.padding_idx\n",
    "            )\n",
    "\n",
    "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
    "            config.max_position_embeddings,\n",
    "            embed_dim,\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [BartEncoderLayer(config) for _ in range(config.encoder_layers)]\n",
    "        )\n",
    "        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _resize_attention_mask(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_ids:\n",
    "        :param embeddings:\n",
    "        :param attention_mask:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # identify the locations of the context_delimiter in each of the input sequence\n",
    "        if type(input_ids) is list:\n",
    "            input_ids = torch.LongTensor(\n",
    "                input_ids,\n",
    "            )\n",
    "        delimiter_points = input_ids == self._context_delimiter_id\n",
    "\n",
    "        delimiter_points_idxs = delimiter_points.nonzero(as_tuple=True)[-1]\n",
    "\n",
    "        \n",
    "        all_attention_masks = []\n",
    "        all_input_ids = []\n",
    "        max_length = 0\n",
    "\n",
    "        # For item in input_ids, embeddings, attention_mask, input_ids, select the\n",
    "        # portion of the tensor after the delimiter_point_id\n",
    "        for delimiter_point_id,  att_mask in zip(\n",
    "            delimiter_points_idxs,attention_mask\n",
    "        ):\n",
    "            \n",
    "            if max_length < att_mask.shape[0]:\n",
    "                max_length = att_mask.shape[0]\n",
    "            \n",
    "            all_attention_masks.append(att_mask[delimiter_point_id + 1 :])\n",
    "\n",
    "        # Reshape all the section of interest for each item in all_input_ids, all_embeddings, all_attention_masks to\n",
    "        # the same size\n",
    "        batch_attention_masks: List = list()\n",
    "\n",
    "        for idx, att_mask in enumerate( all_attention_masks):\n",
    "            len_diff = max_length - att_mask.shape[0]\n",
    "            if max_length > att_mask.shape[0]:\n",
    "\n",
    "                attn_pads = torch.zeros(\n",
    "                    len_diff,\n",
    "                ).to(att_mask.device)\n",
    "                att_mask = torch.concat([att_mask, attn_pads], -1)\n",
    "                \n",
    "            batch_attention_masks += [att_mask.view(-1, max_length)]\n",
    "        \n",
    "        # Create the final tensors with the contexts removed\n",
    "        batch_attention_masks = torch.concat(batch_attention_masks, 0)\n",
    "        return  batch_attention_masks\n",
    "    \n",
    "    def _strip_context(self, input_ids, embeddings, attention_mask):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_ids:\n",
    "        :param embeddings:\n",
    "        :param attention_mask:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # identify the locations of the context_delimiter in each of the input sequence\n",
    "        if type(input_ids) is list:\n",
    "            input_ids = torch.LongTensor(\n",
    "                input_ids,\n",
    "            )\n",
    "        delimiter_points = input_ids == self._context_delimiter_id\n",
    "\n",
    "        delimiter_points_idxs = delimiter_points.nonzero(as_tuple=True)[-1]\n",
    "\n",
    "        all_embeddings = []\n",
    "        all_attention_masks = []\n",
    "        all_input_ids = []\n",
    "        max_length = 0\n",
    "        embedding_dim = embeddings.shape[-1]\n",
    "\n",
    "        # For item in input_ids, embeddings, attention_mask, input_ids, select the\n",
    "        # portion of the tensor after the delimiter_point_id\n",
    "        for delimiter_point_id, embedding, att_mask in zip(\n",
    "            delimiter_points_idxs, embeddings, attention_mask\n",
    "        ):\n",
    "            embedding = embedding[delimiter_point_id + 1 :, :]\n",
    "            if max_length < embedding.shape[0]:\n",
    "                max_length = embedding.shape[0]\n",
    "            all_embeddings.append(embedding)\n",
    "            all_attention_masks.append(att_mask[delimiter_point_id + 1 :])\n",
    "\n",
    "        # Reshape all the section of interest for each item in all_input_ids, all_embeddings, all_attention_masks to\n",
    "        # the same size\n",
    "        batch_embeddings: List = list()\n",
    "        batch_attention_masks: List = list()\n",
    "\n",
    "        for idx, (embedding, att_mask) in enumerate(\n",
    "            zip(all_embeddings, all_attention_masks)\n",
    "        ):\n",
    "            len_diff = max_length - embedding.shape[0]\n",
    "            if max_length > embedding.shape[0]:\n",
    "                pad_tensor = torch.zeros(len_diff, embedding_dim).to(embedding.device)\n",
    "                embedding = torch.concat([embedding, pad_tensor], dim=0)\n",
    "\n",
    "                attn_pads = torch.zeros(\n",
    "                    len_diff,\n",
    "                ).to(att_mask.device)\n",
    "                att_mask = torch.concat([att_mask, attn_pads], -1)\n",
    "\n",
    "            batch_embeddings += [embedding.view(-1, max_length, embedding_dim)]\n",
    "            batch_attention_masks += [att_mask.view(-1, max_length)]\n",
    "        \n",
    "        # Create the final tensors with the contexts removed\n",
    "        batch_attention_masks = torch.concat(batch_attention_masks, 0)\n",
    "        batch_embeddings = torch.concat(batch_embeddings, 0)\n",
    "        return delimiter_points_idxs,batch_embeddings, batch_attention_masks\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        shrink_map: Optional[dict] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
    "                provide it.\n",
    "\n",
    "                Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "                [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "                [What are input IDs?](../glossary#input-ids)\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time\"\n",
    "            )\n",
    "        elif input_ids is not None:\n",
    "            input = input_ids\n",
    "            input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "        elif inputs_embeds is not None:\n",
    "            input = inputs_embeds[:, :, -1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
    "\n",
    "        embed_pos = self.embed_positions(input)\n",
    "\n",
    "        hidden_states = inputs_embeds + embed_pos\n",
    "        hidden_states = self.layernorm_embedding(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(\n",
    "            hidden_states, p=self.dropout, training=self.training\n",
    "        )\n",
    "\n",
    "        attention_mask_ = attention_mask\n",
    "\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask_ = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        # check if head_mask has a correct number of layers specified if desired\n",
    "        if head_mask is not None:\n",
    "            if head_mask.size()[0] != (len(self.layers)):\n",
    "                raise ValueError(\n",
    "                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                    f\" {head_mask.size()[0]}.\"\n",
    "                )\n",
    "                \n",
    "        \n",
    "\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (\n",
    "                dropout_probability < self.layerdrop\n",
    "            ):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(encoder_layer),\n",
    "                        hidden_states,\n",
    "                        attention_mask_,\n",
    "                        (head_mask[idx] if head_mask is not None else None),\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = encoder_layer(\n",
    "                        hidden_states,\n",
    "                        attention_mask_,\n",
    "                        layer_head_mask=(\n",
    "                            head_mask[idx] if head_mask is not None else None\n",
    "                        ),\n",
    "                        output_attentions=output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        delimiter_points_idxs,hidden_states, batch_encoder_attention_masks = self._strip_context(\n",
    "            input_ids, hidden_states, attention_mask\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        if not return_dict:\n",
    "            \n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    encoder_states,\n",
    "                    all_attentions,\n",
    "                    batch_encoder_attention_masks,\n",
    "                    delimiter_points_idxs,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "            \n",
    "        #print(input_ids.shape, hidden_states.shape,batch_encoder_attention_masks.shape, \" The data size or shape\")\n",
    "        \n",
    "        return EncoderOutputs(\n",
    "            last_hidden_state=hidden_states,\n",
    "            hidden_states=encoder_states,\n",
    "            attentions=all_attentions,\n",
    "            cleaned_mask=batch_encoder_attention_masks,\n",
    "            seperation_point=delimiter_points_idxs\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "473435c5caf2da67d3d84349b3ab99ae605588908510e1f3cdf041055f6c21f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
