{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from functools import partial\n",
    "import nltk\n",
    "from src.contextual_bart import ContextualisedBartModel,BartForContextualRecovery,SimplifiedBeamSearch\n",
    "from src.dataset_processor import load_all_data\n",
    "from src.utils import SmartCollator, get_args, setuptokenizer\n",
    "from src.dataset_processor import (\n",
    "    ContextGenerationDataset,\n",
    ")\n",
    "from transformers import BartTokenizer, BartConfig,BartForConditionalGeneration\n",
    "from src.model_utils import CustomTrainer, get_training_arguments\n",
    "import torch\n",
    "from src.config import DATASET_PATH\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "import pickle as pk\n",
    "import torch\n",
    "from transformers import (    AutoTokenizer,\n",
    "          AutoModelForSeq2SeqLM,\n",
    "         LogitsProcessorList,    MinLengthLogitsProcessor, StoppingCriteriaList, MaxLengthCriteria,\n",
    "         TopKLogitsWarper, TemperatureLogitsWarper,BeamSearchScorer,)\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "def generate_tokenizer_and_data(args):\n",
    "\n",
    "    # load the dataset\n",
    "\n",
    "    train_data_packet = load_all_data(DATASET_PATH, mode=\"train\")\n",
    "    test_data_packet = load_all_data(DATASET_PATH, mode=\"dev\")\n",
    "\n",
    "    print(f\"Training Data size: {len(train_data_packet)}\")\n",
    "    print(f\"Training Data size: {len(test_data_packet)}\")\n",
    "\n",
    "    model_base = args.model_base\n",
    "    tokenizer = setuptokenizer(\n",
    "        model_base=model_base,\n",
    "        special_tokens=[],\n",
    "    )\n",
    "    tokenizer.add_tokens([\"[SEP]\"])\n",
    "\n",
    "    train_dataset = ContextGenerationDataset(\n",
    "        tokenizer=tokenizer, nb_records=len(train_data_packet),\n",
    "    )\n",
    "    train_dataset.change_data_mode(1)\n",
    "    train_dataset.set_record(train_data_packet)\n",
    "\n",
    "    test_dataset = ContextGenerationDataset(\n",
    "        tokenizer=tokenizer, nb_records=len(test_data_packet), \n",
    "    )\n",
    "    test_dataset.change_data_mode(1)\n",
    "    test_dataset.set_record(test_data_packet)\n",
    "\n",
    "    return train_dataset, test_dataset, [train_data_packet,test_data_packet]\n",
    "\n",
    "\n",
    "\n",
    "def model_init(\n",
    "    vocab_size,\n",
    "    context_delimiter_id,\n",
    "    model_base=\"facebook/bart-base\",\n",
    "    use_random_restriction=False,\n",
    "    section_prob=(0.25, 0.45),\n",
    "    device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n",
    "):\n",
    "    def build_model():\n",
    "        bart_config = BartConfig.from_pretrained(model_base)\n",
    "        bart_config.context_delimiter_id = context_delimiter_id\n",
    "        bart_config.use_random_restriction = use_random_restriction\n",
    "        bart_config.section_prob = section_prob\n",
    "\n",
    "        generator = BartForContextualRecovery.from_pretrained(\n",
    "            model_base, config=bart_config, ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "        # update the tokens\n",
    "        generator.resize_token_embeddings(vocab_size)  # type: ignore\n",
    "        return generator.to(device)  # type: ignore\n",
    "    return build_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing files:  ['processed_data/context_generation_train.csv']\n",
      "processing files:  ['processed_data/context_generation_dev.csv']\n",
      "Training Data size: 145670\n",
      "Training Data size: 12151\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Args:\n",
    "    model_base: str\n",
    "    \n",
    "args = Args(model_base=\"facebook/bart-base\")\n",
    "train_dataset, test_dataset,[train_data_packet,test_data_packet] = generate_tokenizer_and_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].section_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([83]), torch.Size([84]), 33)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[67].labels.shape,train_dataset[67].input_ids.shape,train_dataset[67].section_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0, 35396, 19336, 16441,  1253,   680, 17089, 31062, 27192,     6,\n",
       "        18603,   661, 11710,  2083,  7864,     6,     8,   559, 34580,  6805,\n",
       "        22327,   405,     4,    20,  9388,  1548,     6, 31217,    30,  9019,\n",
       "         3634, 39321, 17589,  4936,  1343,     8,  1918,  2013,  2636,  2211,\n",
       "            8,   617,  2226,    30,  1030, 20875, 15281,   234,  4781, 19898,\n",
       "            6, 50265,  5087,  8490,    29,  3519,   223, 18668,  2301,    35,\n",
       "            5,   588,    12,  8331,  1460,     7,  1760,     4,  1868,     5,\n",
       "         9388,  1548,     8, 37958,  1809,  3951,  2031,    25,   402,    61,\n",
       "          531,    28,  5032, 22241,     4,     2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[67].input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "context_delimiter_id = train_dataset.tokenizer.get_added_vocab()['[SEP]']\n",
    "\n",
    "train_model_path = \"trained_models_mtl/bart_base_model_section_point/checkpoint-45525/pytorch_model.bin\"\n",
    "\n",
    "generator = model_init(len(train_dataset.tokenizer),\n",
    "                       context_delimiter_id=context_delimiter_id,\n",
    "                       model_base=args.model_base,use_random_restriction=False)()\n",
    "\n",
    "state_dict = torch.load(train_model_path)\n",
    "generator.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ContextGenerationDataset(test_dataset.tokenizer,nb_records=1,use_random_restrictive=True)\n",
    "dataset.change_data_mode(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s>Seven (stylized as Se7en) is a 1990 American crime thriller film directed by David Fincher and written by Andrew Kevin Walker. It stars Brad Pitt, Morgan Freeman, Gwyneth Paltrow, and [SEP] John C. McGinley. Set in a crime-ridden, unnamed city, Seven's plot follows disenchanted, near-retirement detective William Somerset (Freeman) and his new partner, the recently transferred David Mills (Pitt), as they attempt to stop a serial killer before he can complete a series of murders based on the seven deadly sins.</s>\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedBeamSearch:\n",
    "    def __init__(self, generator, tokenizer) -> None:\n",
    "        self.generator = generator\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        num_beams=5,\n",
    "        min_length=100,\n",
    "        max_length=500,\n",
    "        top_k=50,\n",
    "        temperature=0.85,\n",
    "    ):\n",
    "\n",
    "        # initialise decoder input_ids\n",
    "        decoder_input_ids = torch.ones(\n",
    "            (num_beams, 1), device=self.generator.device, dtype=torch.long\n",
    "        )\n",
    "        decoder_input_ids = (\n",
    "            decoder_input_ids * self.generator.config.decoder_start_token_id\n",
    "        )\n",
    "        \n",
    "        encoder_outputs = self.generator.get_encoder()(\n",
    "                input_ids.repeat_interleave(num_beams, dim=0),\n",
    "                attention_mask.repeat_interleave(num_beams, dim=0),\n",
    "                return_dict=True,\n",
    "            )\n",
    "        #print(encoder_outputs[0].shape)\n",
    "\n",
    "        model_kwargs = {\n",
    "            \"encoder_outputs\": encoder_outputs\n",
    "        }\n",
    "        beam_scorer = BeamSearchScorer(\n",
    "            batch_size=attention_mask.shape[0],\n",
    "            num_beams=num_beams,\n",
    "            device=self.generator.device,\n",
    "        )\n",
    "\n",
    "        logits_processor = LogitsProcessorList(\n",
    "            [\n",
    "                MinLengthLogitsProcessor(\n",
    "                    1, eos_token_id=self.generator.config.eos_token_id\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        logits_warper = LogitsProcessorList(\n",
    "            [\n",
    "                TopKLogitsWarper(top_k),\n",
    "                TemperatureLogitsWarper(temperature),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        outputs = self.generator.beam_sample(\n",
    "            decoder_input_ids,\n",
    "            beam_scorer,\n",
    "            max_length=max_length,\n",
    "            logits_processor=logits_processor,\n",
    "            # stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length)),\n",
    "            logits_warper=logits_warper,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "\n",
    "        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, torch.Size([1, 237]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataset_processor import ContextualGenerationData\n",
    "from pytorch_lightning import seed_everything\n",
    "data = ContextualGenerationData(input=\"\"\"\n",
    "                                Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects  they thought they had. Given that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out. \"Surgery ,\" one replied.\n",
    "                                I was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job . One girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\" At that point, I was shocked. I am short, I can\\'t deny that, but I don\\'t think I would put myself through months of agony just to be a few centimetres taller.\n",
    "                                \"\"\".replace(\"\\n\",\"\").strip(),output=\"\")\n",
    "\n",
    "batch = dataset.procesTexts(data)\n",
    "b_input_ids = batch.input_ids.view(1, -1).to(device)\n",
    "b_input_mask = batch.attention_mask.view(1, -1).to(device)\n",
    "batch.section_point, b_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects they thought they had. Given that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a or family friend to help them out. \"Surgery,\" one replied. I was pretty alarmed by that response. It seems that the graduates of [SEP] today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job. One girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\" At that point, I was shocked. I am short, I can\\'t deny that, but I don\\'t think I would put myself through months of agony just to be a few centimetres taller.</s>']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tokenizer.batch_decode(b_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 118, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects they thought they had. Given that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a family or family friend to help them out. \"Surgery,\" one replied. It was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job. One girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\" At that point, I was shocked. I am short, I can\\'t deny that, but I don\\'t think I would put myself through months of agony just to be a few centimetres taller.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(100)\n",
    "bb= SimplifiedBeamSearch(generator,dataset.tokenizer)\n",
    "bb.generate(input_ids=b_input_ids,\n",
    "            attention_mask=b_input_mask,\n",
    "            num_beams=10,\n",
    "            max_length=270,\n",
    "            temperature=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_embedding_sections(batch_size, max_length, low=0.45, high=0.55):\n",
    "    deletion_section_probs = np.random.uniform(size=(batch_size,), low=low, high=high)\n",
    "    deletion_section = max_length * deletion_section_probs\n",
    "    return torch.round(\n",
    "        torch.FloatTensor(deletion_section),\n",
    "    ).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 93, 100, 100, 109,  91,  93, 103,  91,  94, 101, 100,  91, 102, 110,\n",
       "        102])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_embedding_sections(15,200,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from logging import Logger\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    ")\n",
    "from transformers.models.bart.modeling_bart import (\n",
    "    BartConfig,\n",
    "    BartDecoder,\n",
    "    BartEncoderLayer,\n",
    "    BartLearnedPositionalEmbedding,\n",
    "    BartPretrainedModel,\n",
    "    BaseModelOutput,\n",
    "    CrossEntropyLoss,\n",
    "    Seq2SeqLMOutput,\n",
    "    Seq2SeqModelOutput,\n",
    "    _expand_mask,\n",
    "    shift_tokens_right,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EncoderOutputs(BaseModelOutput):\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attention_mask: torch.LongTensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestrictedBartEncoder(BartPretrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
    "    [`BartEncoderLayer`].\n",
    "\n",
    "    Args:\n",
    "        config: BartConfig\n",
    "        embed_tokens (nn.Embedding): output embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "        self.layerdrop = config.encoder_layerdrop\n",
    "\n",
    "        embed_dim = config.d_model\n",
    "        self.padding_idx = config.pad_token_id\n",
    "\n",
    "        #self._context_delimiter_id = config.context_delimiter_id\n",
    "        self._min_section_prob,self._max_section_prob = config.section_prob\n",
    "        self.max_source_positions = config.max_position_embeddings\n",
    "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
    "\n",
    "        if embed_tokens is not None:\n",
    "            self.embed_tokens = embed_tokens\n",
    "        else:\n",
    "            self.embed_tokens = nn.Embedding(\n",
    "                config.vocab_size, embed_dim, self.padding_idx\n",
    "            )\n",
    "\n",
    "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
    "            config.max_position_embeddings,\n",
    "            embed_dim,\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [BartEncoderLayer(config) for _ in range(config.encoder_layers)]\n",
    "        )\n",
    "        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "        \n",
    "    \n",
    "    def _get_random_embedding_sections(self,batch_size, max_length, low=0.20, high=0.6):\n",
    "        deletion_section_probs = np.random.uniform(size=(batch_size,), low=low, high=high)\n",
    "        deletion_section = max_length * deletion_section_probs\n",
    "        return torch.round(\n",
    "            torch.FloatTensor(deletion_section),\n",
    "        ).long()\n",
    "    def _strip_context(self, input_ids, embeddings, attention_mask):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_ids:\n",
    "        :param embeddings:\n",
    "        :param attention_mask:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # identify the locations of the context_delimiter in each of the input sequence\n",
    "        if type(input_ids) is list:\n",
    "            input_ids = torch.LongTensor(\n",
    "                input_ids,\n",
    "            )\n",
    "            \n",
    "        # Get the batch-size and the max_len of embeddings\n",
    "        batch_size, batch_max_length,_ =  embeddings.shape\n",
    "        \n",
    "        #delimiter_points.nonzero(as_tuple=True)[-1]\n",
    "        \n",
    "        # Randomly select parts of the encoder output to \n",
    "        delimiter_points_idxs = self._get_random_embedding_sections(batch_size,\n",
    "                                                                    batch_max_length,\n",
    "                                                                    self._min_section_prob,\n",
    "                                                                    self._max_section_prob)\n",
    "\n",
    "        all_embeddings = []\n",
    "        all_attention_masks = []\n",
    "        all_input_ids = []\n",
    "        max_length = 0\n",
    "        embedding_dim = embeddings.shape[-1]\n",
    "\n",
    "        # For item in input_ids, embeddings, attention_mask, input_ids, select the\n",
    "        # portion of the tensor after the delimiter_point_id\n",
    "        for delimiter_point_id, embedding, att_mask in zip(\n",
    "            delimiter_points_idxs, embeddings, attention_mask\n",
    "        ):\n",
    "            embedding = embedding[delimiter_point_id + 1 :, :]\n",
    "            if max_length < embedding.shape[0]:\n",
    "                max_length = embedding.shape[0]\n",
    "            all_embeddings.append(embedding)\n",
    "            all_attention_masks.append(att_mask[delimiter_point_id + 1 :])\n",
    "\n",
    "        # Reshape all the section of interest for each item in all_input_ids, all_embeddings, all_attention_masks to\n",
    "        # the same size\n",
    "        batch_embeddings: List = list()\n",
    "        batch_attention_masks: List = list()\n",
    "\n",
    "        for idx, (embedding, att_mask) in enumerate(\n",
    "            zip(all_embeddings, all_attention_masks)\n",
    "        ):\n",
    "            len_diff = max_length - embedding.shape[0]\n",
    "            if max_length > embedding.shape[0]:\n",
    "                pad_tensor = torch.zeros(len_diff, embedding_dim).to(embedding.device)\n",
    "                embedding = torch.concat([embedding, pad_tensor], dim=0)\n",
    "\n",
    "                attn_pads = torch.zeros(\n",
    "                    len_diff,\n",
    "                ).to(att_mask.device)\n",
    "                att_mask = torch.concat([att_mask, attn_pads], -1)\n",
    "\n",
    "            batch_embeddings += [embedding.view(-1, max_length, embedding_dim)]\n",
    "            batch_attention_masks += [att_mask.view(-1, max_length)]\n",
    "        \n",
    "        # Create the final tensors with the contexts removed\n",
    "        batch_attention_masks = torch.concat(batch_attention_masks, 0)\n",
    "        batch_embeddings = torch.concat(batch_embeddings, 0)\n",
    "        return batch_embeddings, batch_attention_masks\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
    "                provide it.\n",
    "\n",
    "                Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "                [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "                [What are input IDs?](../glossary#input-ids)\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time\"\n",
    "            )\n",
    "        elif input_ids is not None:\n",
    "            input = input_ids\n",
    "            input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "        elif inputs_embeds is not None:\n",
    "            input = inputs_embeds[:, :, -1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
    "\n",
    "        embed_pos = self.embed_positions(input)\n",
    "\n",
    "        hidden_states = inputs_embeds + embed_pos\n",
    "        hidden_states = self.layernorm_embedding(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(\n",
    "            hidden_states, p=self.dropout, training=self.training\n",
    "        )\n",
    "\n",
    "        attention_mask_ = attention_mask\n",
    "\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask_ = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        # check if head_mask has a correct number of layers specified if desired\n",
    "        if head_mask is not None:\n",
    "            if head_mask.size()[0] != (len(self.layers)):\n",
    "                raise ValueError(\n",
    "                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                    f\" {head_mask.size()[0]}.\"\n",
    "                )\n",
    "\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (\n",
    "                dropout_probability < self.layerdrop\n",
    "            ):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(encoder_layer),\n",
    "                        hidden_states,\n",
    "                        attention_mask_,\n",
    "                        (head_mask[idx] if head_mask is not None else None),\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = encoder_layer(\n",
    "                        hidden_states,\n",
    "                        attention_mask_,\n",
    "                        layer_head_mask=(\n",
    "                            head_mask[idx] if head_mask is not None else None\n",
    "                        ),\n",
    "                        output_attentions=output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        hidden_states, batch_encoder_attention_masks = self._strip_context(\n",
    "            input_ids, hidden_states, attention_mask\n",
    "        )\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    encoder_states,\n",
    "                    all_attentions,\n",
    "                    batch_encoder_attention_masks,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "\n",
    "        return EncoderOutputs(\n",
    "            last_hidden_state=hidden_states,\n",
    "            hidden_states=encoder_states,\n",
    "            attentions=all_attentions,\n",
    "            attention_mask=batch_encoder_attention_masks,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "config = copy.deepcopy(generator.config)\n",
    "config.section_prob = (0.2,0.65)\n",
    "config.context_delimiter_id = generator.model.get_encoder()._context_delimiter_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model.get_encoder()._context_delimiter_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restrictive_encoder = RestrictedBartEncoder.from_pretrained(\"facebook/bart-base\",config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouut = restrictive_encoder(b_input_ids.repeat_interleave(4, dim=0),b_input_mask.repeat_interleave(4, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 121])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"race\",'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc=dataset['train'].features[\"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects  they thought they had. Given that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out. \"Surgery ,\" one replied. I was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job . One girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\" At that point, I was shocked. I am short, I can\\'t deny that, but I don\\'t think I would put myself through months of agony just to be a few centimetres taller. I don\\'t even bother to wear shoes with thick soles, as I\\'m not trying to hide the fact that I am just not tall! It seems to me that there is a trend towards wanting \"perfection\" , and that is an ideal that just does not exist in reality. No one is born perfect, yet magazines, TV shows and movies present images of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics fill the pages of newspapers, further creating an idea that \"perfection\" is a requirement, and that it must be purchased, no matter what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][1]['article'].replace('\\n',' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "473435c5caf2da67d3d84349b3ab99ae605588908510e1f3cdf041055f6c21f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
