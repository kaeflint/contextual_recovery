{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "import numpy as np\n",
    "def contextualized_texts(texts):\n",
    "    sentences = sent_tokenize(texts)\n",
    "    data = []\n",
    "    if len(sentences)<3:\n",
    "        return []\n",
    "    else:\n",
    "        \n",
    "        for i in range(len(sentences)-1):\n",
    "            intp = ' '.join(sentences[:i+1][-3:]) + ' [SEP] ' + ' '.join(sentences[i+1:][:3])\n",
    "            output = ' '.join(sentences[:i+1][-3:])\n",
    "            data.append(dict(input=intp,output=output))\n",
    "        return data\n",
    "            \n",
    "def contextualized_data(dataset):\n",
    "    data_pack = []\n",
    "    for data in dataset:\n",
    "        pack = contextualized_texts(data)\n",
    "        if len(pack)>3:\n",
    "            pack = random.sample(pack,3)\n",
    "        data_pack+=pack\n",
    "        \n",
    "    data_pack = pd.DataFrame(data_pack)\n",
    "    data_pack= data_pack.drop_duplicates(subset = [\"output\"],keep=\"last\")\n",
    "    return data_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train_data = pd.read_csv(\"curated_data/squad_train.csv\")\n",
    "squad_dev_data = pd.read_csv(\"curated_data/squad_dev.csv\")\n",
    "drop_train_data = pd.read_csv(\"curated_data/drop_train.csv\")\n",
    "drop_dev_data = pd.read_csv(\"curated_data/drop_dev.csv\")\n",
    "extra_train_data = pd.read_csv(\"curated_data/extra_data_train.csv\")\n",
    "sci_train_data = pd.read_csv(\"curated_data/sci_train.csv\")\n",
    "sci_dev_data = pd.read_csv(\"curated_data/sci_dev.csv\")\n",
    "\n",
    "rope_train_data = pd.read_csv(\"curated_data/rope_train.csv\")\n",
    "rope_dev_data = pd.read_csv(\"curated_data/rope_dev.csv\")\n",
    "\n",
    "\n",
    "squad_train_text = squad_train_data.input_text.unique()\n",
    "squad_dev_text = squad_dev_data.input_text.unique()\n",
    "drop_train_text = drop_train_data.input_text.unique()\n",
    "drop_dev_text = drop_dev_data.input_text.unique()\n",
    "extra_train_text = extra_train_data.input_text.unique()\n",
    "sci_train_text = sci_train_data.input_text.unique()\n",
    "rope_train_text = rope_train_data.input_text.unique()\n",
    "rope_dev_text = rope_dev_data.input_text.unique()\n",
    "train_data_list = np.concatenate([extra_train_text,\n",
    "                                  sci_train_text,\n",
    "                                  squad_train_text,\n",
    "                                  drop_train_text,\n",
    "                                  rope_train_text])\n",
    "\n",
    "dev_data_list = np.concatenate([squad_dev_text,drop_dev_text,rope_dev_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "squad_train_dataset = contextualized_data(squad_train_text)\n",
    "squad_dev_dataset = contextualized_data(squad_dev_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "drop_train_dataset = contextualized_data(drop_train_text)\n",
    "drop_dev_dataset = contextualized_data(drop_dev_text)\n",
    "\n",
    "\n",
    "\n",
    "extra_train_dataset = contextualized_data(extra_train_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sci_dev_text = sci_dev_data.input_text.unique()\n",
    "\n",
    "sci_train_dataset = contextualized_data(sci_train_text)\n",
    "sci_dev_dataset = contextualized_data(sci_dev_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rope_train_dataset = contextualized_data(rope_train_text)\n",
    "rope_dev_dataset = contextualized_data(rope_dev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6642,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([squad_train_dataset,drop_train_dataset,extra_train_dataset,sci_train_dataset,rope_train_dataset],axis=0)\n",
    "dev_data = pd.concat([squad_dev_dataset,drop_dev_dataset,sci_dev_dataset,rope_dev_dataset],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('processed_data/context_generation_train.csv')\n",
    "dev_data.to_csv('processed_data/context_generation_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def strip_newline(value):\n",
    "    return ' '.join(value.splitlines())\n",
    "rc_test= [strip_newline(d['plot']) for d in json.load(open('curated_data//ParaphraseRC_test.json',encoding=\"utf-8\"))]\n",
    "rc_train= [strip_newline(d['plot'])  for d in json.load(open('curated_data//ParaphraseRC_tr.json',encoding=\"utf-8\"))]\n",
    "rc_dev =  [strip_newline(d['plot']) for d in json.load(open('curated_data//ParaphraseRC_dev.json',encoding=\"utf-8\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import ngrams\n",
    "def normalize_whitespace(string):\n",
    "    return re.sub(r\"(\\s)\\1{1,}\", r\"\\1\", string)\n",
    "\n",
    "\n",
    "def cleanDocument(document):\n",
    "    document = re.sub(r\"\\[\\d+\\s?\\]\", \"\", document)\n",
    "    document = re.sub(r\"(\\d\\.\\s+|[a-z]\\)\\s+|•\\s+|[A-Z]\\.\\s+|[IVX]+\\.\\s+)\", \"\", document)\n",
    "    document = normalize_whitespace(document.replace(\"\\n\", \"\")).strip()\n",
    "    return document\n",
    "def collateData(data):\n",
    "    data_pack = []\n",
    "    for texts in data:\n",
    "        texts = cleanDocument(texts)\n",
    "        sent_list = sent_tokenize(texts)\n",
    "        \n",
    "        if len(sent_list)>3:\n",
    "            sentences_pack = [' '.join(l) for l in list(ngrams(sent_list,3))]\n",
    "        else:\n",
    "            sentences_pack = [' '.join(sent_list)]\n",
    "        for ss in sentences_pack:\n",
    "            a= dict(input=ss,output=ss)\n",
    "            data_pack.append(a)\n",
    "    data_pack = pd.DataFrame(data_pack)\n",
    "    data_pack= data_pack.drop_duplicates(subset = [\"output\"],keep=\"last\")\n",
    "    return data_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualized_texts(rc_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_train_dataset = pd.concat([collateData(rc_train) ,\n",
    "                              collateData(train_data_list),\n",
    "                              collateData(rc_dev+rc_test[:800])],axis=0)\n",
    "rc_dev_dataset = pd.concat([collateData(rc_test[800:]),\n",
    "                            collateData(dev_data_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_train_dataset.to_csv('processed_data_new/context_generation_train.csv')\n",
    "rc_dev_dataset.to_csv('processed_data_new/context_generation_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A 2nd grade classroom photo was held today.',\n",
       " 'They all gathered by age.',\n",
       " 'Jeremy was the oldest.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(rc_dev_dataset.input.values[-800:][500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = collateData(dev_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28957, 2), (526386, 2))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc_dev_dataset.shape,rc_train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Set in the second half of the 22nd century, Mars has been 84% terraformed, allowing humans to walk on the surface without pressure suits.',\n",
       " 'Martian society has become matriarchal, with women in most positions of authority.',\n",
       " 'The story concerns police officer Melanie Ballard (Natasha Henstridge), second in command of a team alongside Sergeant Jericho (Jason Statham) sent to a remote mining outpost to transport prisoner Desolation Williams (Ice Cube).',\n",
       " 'Arriving at the remote mining town, Ballard finds all of the people missing.',\n",
       " 'She learns that they had discovered an underground doorway created by an ancient Martian civilization.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(rc_train_dataset.input.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_train_dataset.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(526386, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc_train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.Tensor([1,42,3,56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#url#. #url#. htm.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern =  r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "\n",
    "sent=\"http://classes. midlandstech.edu/carterp/courses/bio225/chap05/lecture1. htm.\" \n",
    "re.sub(pattern,\"#url#\",sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to find the URL from an input string\n",
    "# Using the regular expression\n",
    "import re\n",
    "\n",
    "\n",
    "def Find(string):\n",
    "\n",
    "\t# findall() has been used\n",
    "\t# with valid conditions for urls in string\n",
    "\tregex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "\turl = re.findall(regex, string)\n",
    "\treturn [x[0] for x in url]\n",
    "\n",
    "\n",
    "# Driver Code\n",
    "string = 'My Profile: https://auth.geeksforgeeks.org/user/Chinmoy%20Lenka/articles in the portal of https://www.geeksforgeeks.org/'\n",
    "print(\"Urls: \", Find(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library import ContextualizedSentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sentence_transformer = ContextualizedSentenceTransformer(model_name='sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, BertGenerationDecoder\n",
    "from transformers.models.bart.modeling_bart import BartDecoder\n",
    "from transformers import BartTokenizer\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = BartTokenizer.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder = BartDecoder.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualGenerator(nn.Module):\n",
    "    def __init__(self, decoder_model,sentence_transformer_model= 'sentence-transformers/all-mpnet-base-v2') -> None:\n",
    "        super().__init__()\n",
    "        self._context_sentence_encoder = ContextualizedSentenceTransformer(model_name=sentence_transformer_model,\n",
    "                                                                           clean_context=True)\n",
    "        self._decoder_model = decoder_model\n",
    "    \n",
    "    def forward(self, input_seq,\n",
    "                attention_mask,\n",
    "                label, \n",
    "                decoder_attention_mask)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_trainer = Seq2SeqTrainer(model=context_sentence_transformer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to context_encoder_model/encoder_model\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"
     ]
    }
   ],
   "source": [
    "m_trainer.save_model('context_encoder_model/encoder_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00321b0916143908f5ce3a50d7431c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/nlplab/.cache/huggingface/hub/models--sentence-transformers--all-mpnet-base-v2/snapshots/bd44305fd6a1b43c16baf96765e2ecb20bca8e1d/config.json\n",
      "Model config MPNetConfig {\n",
      "  \"_name_or_path\": \"microsoft/mpnet-base\",\n",
      "  \"architectures\": [\n",
      "    \"MPNetForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"mpnet\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30527\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import MPNetConfig\n",
    "config = MPNetConfig.from_pretrained('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in context_encoder_model/encoder_model/config.json\n"
     ]
    }
   ],
   "source": [
    "config.save_pretrained('context_encoder_model/encoder_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(context_sentence_transformer.state_dict,'context_encoder_model/context_encoder.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(encoder_pretrained_model_name_or_path='context_encoder_model/encoder_model',\n",
    "                                                            decoder_pretrained_model_name_or_path='roberta-base'\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_data=[\"Hello world </s> Mate was there last week\"]\n",
    "cc= context_sentence_transformer.tokenize(chat_data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method forward in module transformers.models.mpnet.modeling_mpnet:\n",
      "\n",
      "forward(input_ids: Optional[torch.LongTensor] = None, attention_mask: Optional[torch.FloatTensor] = None, position_ids: Optional[torch.LongTensor] = None, head_mask: Optional[torch.FloatTensor] = None, inputs_embeds: Optional[torch.FloatTensor] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None, **kwargs) -> Union[Tuple[torch.Tensor], transformers.modeling_outputs.BaseModelOutputWithPooling] method of transformers.models.mpnet.modeling_mpnet.MPNetModel instance\n",
      "    The [`MPNetModel`] forward method, overrides the `__call__` special method.\n",
      "    \n",
      "    <Tip>\n",
      "    \n",
      "    Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "    instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "    the latter silently ignores them.\n",
      "    \n",
      "    </Tip>\n",
      "    \n",
      "    Args:\n",
      "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "            Indices of input sequence tokens in the vocabulary.\n",
      "    \n",
      "            Indices can be obtained using [`MPNetTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "            [`PreTrainedTokenizer.__call__`] for details.\n",
      "    \n",
      "            [What are input IDs?](../glossary#input-ids)\n",
      "        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      "    \n",
      "            - 1 for tokens that are **not masked**,\n",
      "            - 0 for tokens that are **masked**.\n",
      "    \n",
      "            [What are attention masks?](../glossary#attention-mask)\n",
      "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
      "            config.max_position_embeddings - 1]`.\n",
      "    \n",
      "            [What are position IDs?](../glossary#position-ids)\n",
      "        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
      "            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
      "    \n",
      "            - 1 indicates the head is **not masked**,\n",
      "            - 0 indicates the head is **masked**.\n",
      "    \n",
      "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
      "            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n",
      "            model's internal embedding lookup matrix.\n",
      "        output_attentions (`bool`, *optional*):\n",
      "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "            tensors for more detail.\n",
      "        output_hidden_states (`bool`, *optional*):\n",
      "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "            more detail.\n",
      "        return_dict (`bool`, *optional*):\n",
      "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "    \n",
      "    Returns:\n",
      "        [`transformers.modeling_outputs.BaseModelOutputWithPooling`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.BaseModelOutputWithPooling`] or a tuple of\n",
      "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "        elements depending on the configuration ([`MPNetConfig`]) and inputs.\n",
      "    \n",
      "        - **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-states at the output of the last layer of the model.\n",
      "        - **pooler_output** (`torch.FloatTensor` of shape `(batch_size, hidden_size)`) -- Last layer hidden-state of the first token of the sequence (classification token) after further processing\n",
      "          through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns\n",
      "          the classification token after processing through a linear layer and a tanh activation function. The linear\n",
      "          layer weights are trained from the next sentence prediction (classification) objective during pretraining.\n",
      "        - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "          one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "    \n",
      "          Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
      "        - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "    \n",
      "          Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "          heads.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```python\n",
      "    >>> from transformers import MPNetTokenizer, MPNetModel\n",
      "    >>> import torch\n",
      "    \n",
      "    >>> tokenizer = MPNetTokenizer.from_pretrained(\"microsoft/mpnet-base\")\n",
      "    >>> model = MPNetModel.from_pretrained(\"microsoft/mpnet-base\")\n",
      "    \n",
      "    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "    >>> outputs = model(**inputs)\n",
      "    \n",
      "    >>> last_hidden_states = outputs.last_hidden_state\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.encoder.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat=model.encoder(**cc,clean_context=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cnn_dailymail/3.0.0 to /home/nlplab/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a0a4924684431295ece39003115cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1b743748124383b062e99409abea54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/159M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e2adb9caaf495d81a0198bf86d88d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/376M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f72fed668644e288bacfdfb9a70333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea307ea70624eca94c6911e2269faf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/661k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388cc914f23341c2a856331bd985c757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/572k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d318a7fd3ac147b3a99141abac8f3540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b509860205c4e79bee26d5d712fed71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3b88eebd2945b2a5fe163736aa0be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cnn_dailymail downloaded and prepared to /home/nlplab/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f11f2c28534cd99ac47f3cc4e1bcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\",'3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.',\n",
       " 'highlights': \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\",\n",
       " 'id': '42c027e4ff9730fbb3de84c1af0d2c506e41c3e4'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "473435c5caf2da67d3d84349b3ab99ae605588908510e1f3cdf041055f6c21f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
