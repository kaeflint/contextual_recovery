{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from functools import partial\n",
    "import nltk\n",
    "from src.contextual_bart import ContextualisedBartModel,BartForContextualRecovery,SimplifiedBeamSearch\n",
    "from src.dataset_processor import load_all_data\n",
    "from src.utils import SmartCollator, get_args, setuptokenizer\n",
    "from src.dataset_processor import (\n",
    "    ContextGenerationDataset,\n",
    ")\n",
    "from transformers import BartTokenizer, BartConfig,BartForConditionalGeneration\n",
    "from src.model_utils import CustomTrainer, get_training_arguments\n",
    "import torch\n",
    "from src.config import DATASET_PATH\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "import pickle as pk\n",
    "import torch\n",
    "from transformers import (    AutoTokenizer,\n",
    "          AutoModelForSeq2SeqLM,\n",
    "         LogitsProcessorList,    MinLengthLogitsProcessor, StoppingCriteriaList, MaxLengthCriteria,\n",
    "         TopKLogitsWarper, TemperatureLogitsWarper,BeamSearchScorer,)\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "DATASET_PATH = \"summarisation_data/\"\n",
    "\n",
    "def generate_tokenizer_and_data(args):\n",
    "\n",
    "    # load the dataset\n",
    "\n",
    "    train_data_packet = load_all_data(DATASET_PATH, mode=\"train\")\n",
    "    dev_data_packet = load_all_data(DATASET_PATH, mode=\"dev\")\n",
    "    test_data_packet = load_all_data(DATASET_PATH,mode=\"test\")\n",
    "\n",
    "    print(f\"Training Data size: {len(train_data_packet)}\")\n",
    "    print(f\"Training Data size: {len(test_data_packet)}\")\n",
    "\n",
    "    model_base = args.model_base\n",
    "    tokenizer = setuptokenizer(\n",
    "        model_base=model_base,\n",
    "        special_tokens=[],\n",
    "    )\n",
    "    tokenizer.add_tokens([args.sep_token])\n",
    "\n",
    "    train_dataset = ContextGenerationDataset(\n",
    "        tokenizer=tokenizer, nb_records=len(train_data_packet), max_len=720,\n",
    "        context_seperator=args.sep_token,\n",
    "        is_auto_encoder_data=not args.is_not_auto_encoder_data,\n",
    "        use_special_token=True,\n",
    "    )\n",
    "    train_dataset.change_data_mode(1)\n",
    "    train_dataset.set_record(train_data_packet)\n",
    "\n",
    "    test_dataset = ContextGenerationDataset(\n",
    "        tokenizer=tokenizer, nb_records=len(test_data_packet), \n",
    "        max_len=700,\n",
    "        context_seperator=args.sep_token,\n",
    "        is_auto_encoder_data=not args.is_not_auto_encoder_data,\n",
    "    )\n",
    "    test_dataset.change_data_mode(1)\n",
    "    test_dataset.set_record(test_data_packet)\n",
    "    \n",
    "    dev_dataset = ContextGenerationDataset(\n",
    "        tokenizer=tokenizer, nb_records=len(dev_data_packet), \n",
    "        max_len=700,\n",
    "        context_seperator=args.sep_token,\n",
    "        is_auto_encoder_data=not args.is_not_auto_encoder_data,\n",
    "    )\n",
    "    test_dataset.change_data_mode(1)\n",
    "    test_dataset.set_record(test_data_packet)\n",
    "\n",
    "    return train_dataset, dev_dataset,test_dataset, [train_data_packet,dev_data_packet,test_data_packet]\n",
    "\n",
    "\n",
    "\n",
    "def model_init(\n",
    "    vocab_size,\n",
    "    context_delimiter_id,\n",
    "    model_base=\"facebook/bart-base\",\n",
    "    use_random_restriction=False,\n",
    "    section_prob=(0.25, 0.45),\n",
    "    device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n",
    "):\n",
    "    def build_model():\n",
    "        bart_config = BartConfig.from_pretrained(model_base)\n",
    "        bart_config.context_delimiter_id = context_delimiter_id\n",
    "        bart_config.use_random_restriction = use_random_restriction\n",
    "        bart_config.section_prob = section_prob\n",
    "\n",
    "        generator = BartForContextualRecovery.from_pretrained(\n",
    "            model_base, config=bart_config, ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "        # update the tokens\n",
    "        generator.resize_token_embeddings(vocab_size)  # type: ignore\n",
    "        return generator.to(device)  # type: ignore\n",
    "    return build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing files:  ['summarisation_data/xsum_train.csv']\n",
      "processing files:  ['summarisation_data/xsum_dev.csv']\n",
      "processing files:  ['summarisation_data/xsum_test.csv']\n",
      "Training Data size: 203083\n",
      "Training Data size: 11322\n",
      "The model will be trained as a non auto-encoder\n",
      "The model will be trained as a non auto-encoder\n",
      "The model will be trained as a non auto-encoder\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Args:\n",
    "    model_base: str\n",
    "    sep_token: str = \"[SEP]\"\n",
    "    is_not_auto_encoder_data: bool = True\n",
    "    \n",
    "    \n",
    "args = Args(model_base=\"facebook/bart-base\")\n",
    "train_dataset, dev_dataset,test_dataset, [train_data_packet, \n",
    "                                          dev_data_packet, \n",
    "                                          test_data_packet] = generate_tokenizer_and_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_delimiter_id = train_dataset.tokenizer.get_vocab()['[SEP]']\n",
    "\n",
    "train_model_path = \"trained_models_sum/bart_base_model_full_e10a4/checkpoint-33848/pytorch_model.bin\"\n",
    "#\"trained_models_mtl/bart_base_model_full/checkpoint-263195/pytorch_model.bin\"\n",
    "\n",
    "generator = model_init(len(train_dataset.tokenizer),\n",
    "                       context_delimiter_id=context_delimiter_id,\n",
    "                       model_base=args.model_base,use_random_restriction=False)()\n",
    "\n",
    "state_dict = torch.load(train_model_path)\n",
    "generator.load_state_dict(state_dict)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be trained as a non auto-encoder\n"
     ]
    }
   ],
   "source": [
    "dataset = ContextGenerationDataset(test_dataset.tokenizer,\n",
    "                                   nb_records=1,\n",
    "                                   section_boundary=(0.4,0.48),\n",
    "                                   \n",
    "        context_seperator=args.sep_token,\n",
    "        is_auto_encoder_data=not args.is_not_auto_encoder_data,\n",
    "                                   use_random_restrictive=True)\n",
    "dataset.change_data_mode(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, torch.Size([1, 147]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataset_processor import ContextualGenerationData\n",
    "from pytorch_lightning import seed_everything\n",
    "data = ContextualGenerationData(input=\"\"\"\n",
    "                                We are helping the community work together towards the goal of advancing Machine Learning.\n",
    "The Hugging Face Hub is a platform with over 60K models, 6K datasets, and 6K demos in which people can easily collaborate in their ML workflows. \n",
    "The Hub works as a central place where anyone can share, explore, discover, and experiment with open-source Machine Learning.\n",
    " No single company, will be able to “solve AI” by themselves - the only way we'll achieve this is by sharing knowledge and resources in a community-centric approach. We are building the largest open-source collection of models, datasets, demos and metrics on the Hugging Face Hub to democratize and advance ML for everyone.\n",
    "                                \"\"\".replace(\"\\n\",\"\").strip(),output=\"\")\n",
    "kk= 45\n",
    "batch = dataset.procesTexts(data)\n",
    "b_input_ids = batch.input_ids.view(1, -1).to(device)\n",
    "b_input_mask = batch.attention_mask.view(1, -1).to(device)\n",
    "batch.section_point, b_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Hugging Face Hub, an open-source machine learning platform for artificial intelligence (AI), has been launched by Microsoft.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb=generator.generate(input_ids=b_input_ids,\n",
    "            attention_mask=b_input_mask,\n",
    "            eos_token_id=test_dataset.tokenizer.eos_token_id,\n",
    "    max_new_tokens=320,\n",
    "    length_penalty=2.95,\n",
    "    early_stopping=True,\n",
    "    use_cache=True,\n",
    "    num_beams=15,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.86,)\n",
    "test_dataset.tokenizer.batch_decode(bb,clean_up_tokenization_spaces=True,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Work to rebuild a shopping centre car park that has been beset with structural problems for a decade will now go ahead in the spring of 2015.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_packet[0].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 944/944 [10:45<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,SequentialSampler\n",
    "import tqdm\n",
    "test_data_loader = DataLoader(test_dataset,batch_size=12,\n",
    "                              sampler= SequentialSampler(test_dataset),\n",
    "                              collate_fn= SmartCollator(\n",
    "            pad_token_id=train_dataset.tokenizer.pad_token_id, max_len=700\n",
    "        )\n",
    "                              )\n",
    "\n",
    "output_summaries2 =[]\n",
    "for batch in tqdm.tqdm(test_data_loader):\n",
    "    b_input_ids = batch['input_ids'].to(device)\n",
    "    b_input_mask = batch['attention_mask'].to(device)\n",
    "    bb=generator.generate(input_ids=b_input_ids,\n",
    "            attention_mask=b_input_mask,\n",
    "            num_beams=10,\n",
    "            do_sample=False,\n",
    "            num_return_sequences=1,\n",
    "            max_new_tokens=300)\n",
    "    \n",
    "    sentences = test_dataset.tokenizer.batch_decode(bb,\n",
    "                                                    clean_up_tokenization_spaces=True,\n",
    "                                                    skip_special_tokens=True)\n",
    "    output_summaries2+=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import writeToFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "writeToFile(output_summaries2, \"summarisation_data/first_large_response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [c.output for c in test_data_packet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nine endangered animals are to be confiscated from a Ceredigion zoo after its owners admitted displaying animals without the proper paperwork.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Two people have been charged with illegal use of endangered animals at a wildlife park in Powys.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_summaries2[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies in the southern Indian state of Tamil Nadu have declared a three-day holiday in honour of popular actor Rajinikanth.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_summaries2[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "metrics = evaluate.combine(['bleu','meteor',\"rouge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = metrics.compute(predictions=output_summaries2,references=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = metrics.compute(predictions=output_summaries2,references=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.1070881814881393,\n",
       " 'precisions': [0.4005549408174899,\n",
       "  0.14972146685323548,\n",
       "  0.07613225896082246,\n",
       "  0.041814503122892414],\n",
       " 'brevity_penalty': 0.9110266635545823,\n",
       " 'length_ratio': 0.914759830685234,\n",
       " 'translation_length': 238584,\n",
       " 'reference_length': 260816,\n",
       " 'meteor': 0.33018881124114496,\n",
       " 'rouge1': 0.3758289918033365,\n",
       " 'rouge2': 0.1604144976545041,\n",
       " 'rougeL': 0.30460972993066676,\n",
       " 'rougeLsum': 0.30463640291568106}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.1153611394841785,\n",
       " 'precisions': [0.43606276592370397,\n",
       "  0.17328569859245072,\n",
       "  0.09197218870077607,\n",
       "  0.052734741092611136],\n",
       " 'brevity_penalty': 0.8337642518091675,\n",
       " 'length_ratio': 0.8461635789215386,\n",
       " 'translation_length': 220693,\n",
       " 'reference_length': 260816,\n",
       " 'meteor': 0.3402536091126836,\n",
       " 'rouge1': 0.39375630986738397,\n",
       " 'rouge2': 0.17592815130727232,\n",
       " 'rougeL': 0.3224054386956998,\n",
       " 'rougeLsum': 0.32241354313777904}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.11396363055865384,\n",
       " 'precisions': [0.4390870861156681,\n",
       "  0.1726855491963649,\n",
       "  0.0905894799313628,\n",
       "  0.05122320492707356],\n",
       " 'brevity_penalty': 0.8321056722435886,\n",
       " 'length_ratio': 0.8447402578391092,\n",
       " 'translation_length': 173116,\n",
       " 'reference_length': 204934,\n",
       " 'meteor': 0.338673154674504,\n",
       " 'rouge1': 0.39394898657436117,\n",
       " 'rouge2': 0.1743277087302274,\n",
       " 'rougeL': 0.3234069211449011,\n",
       " 'rougeLsum': 0.3235298412651805}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Hugging Face Hub, an open-source machine learning platform for artificial intelligence (AI), is being launched today.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz= test_dataset.tokenizer([\"He was there [SEP] but left later [SEP] but nope\",\n",
    "                            \"Nope she kept the car [SEP] his mum [SEP] came home and [SEP] mars\"],\n",
    "                           padding=True,\n",
    "                           \n",
    "                           return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = zz['input_ids']\n",
    "attn = zz['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter_points = input_ids == generator.model.encoder._context_delimiter_id\n",
    "\n",
    "delimiter_points_idxs = delimiter_points.nonzero(as_tuple=True)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  8,  7, 10, 15])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delimiter_points_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "473435c5caf2da67d3d84349b3ab99ae605588908510e1f3cdf041055f6c21f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
