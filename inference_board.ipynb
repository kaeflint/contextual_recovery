{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from functools import partial\n",
    "import nltk\n",
    "from src.contextual_bart import ContextualisedBartModel,BartForContextualRecovery,SimplifiedBeamSearch\n",
    "from src.dataset_processor import load_all_data\n",
    "from src.utils import SmartCollator, get_args, setuptokenizer\n",
    "from src.dataset_processor import (\n",
    "    ContextGenerationDataset,\n",
    ")\n",
    "from transformers import BartTokenizer, BartConfig,BartForConditionalGeneration\n",
    "from src.model_utils import CustomTrainer, get_training_arguments\n",
    "import torch\n",
    "from src.config import DATASET_PATH\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "import pickle as pk\n",
    "import torch\n",
    "from transformers import (    AutoTokenizer,\n",
    "          AutoModelForSeq2SeqLM,\n",
    "         LogitsProcessorList,    MinLengthLogitsProcessor, StoppingCriteriaList, MaxLengthCriteria,\n",
    "         TopKLogitsWarper, TemperatureLogitsWarper,BeamSearchScorer,)\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "DATASET_PATH = \"summarisation_data/\"\n",
    "\n",
    "def generate_tokenizer_and_data(args):\n",
    "\n",
    "    # load the dataset\n",
    "\n",
    "    train_data_packet = load_all_data(DATASET_PATH, mode=\"train\")\n",
    "    dev_data_packet = load_all_data(DATASET_PATH, mode=\"dev\")\n",
    "    test_data_packet = load_all_data(DATASET_PATH,mode=\"test\")\n",
    "\n",
    "    print(f\"Training Data size: {len(train_data_packet)}\")\n",
    "    print(f\"Training Data size: {len(test_data_packet)}\")\n",
    "\n",
    "    model_base = args.model_base\n",
    "    tokenizer = setuptokenizer(\n",
    "        model_base=model_base,\n",
    "        special_tokens=[],\n",
    "    )\n",
    "    tokenizer.add_tokens([args.sep_token])\n",
    "\n",
    "    train_dataset = ContextGenerationDataset(\n",
    "        tokenizer=tokenizer, nb_records=len(train_data_packet), max_len=720,\n",
    "        context_seperator=args.sep_token,\n",
    "        is_auto_encoder_data=not args.is_not_auto_encoder_data,\n",
    "        use_special_token=True,\n",
    "    )\n",
    "    train_dataset.change_data_mode(1)\n",
    "    train_dataset.set_record(train_data_packet)\n",
    "\n",
    "    test_dataset = ContextGenerationDataset(\n",
    "        tokenizer=tokenizer, nb_records=len(test_data_packet), \n",
    "        max_len=700,\n",
    "        context_seperator=args.sep_token,\n",
    "        is_auto_encoder_data=not args.is_not_auto_encoder_data,\n",
    "    )\n",
    "    test_dataset.change_data_mode(1)\n",
    "    test_dataset.set_record(test_data_packet)\n",
    "    \n",
    "    dev_dataset = ContextGenerationDataset(\n",
    "        tokenizer=tokenizer, nb_records=len(dev_data_packet), \n",
    "        max_len=700,\n",
    "        context_seperator=args.sep_token,\n",
    "        is_auto_encoder_data=not args.is_not_auto_encoder_data,\n",
    "    )\n",
    "    test_dataset.change_data_mode(1)\n",
    "    test_dataset.set_record(test_data_packet)\n",
    "\n",
    "    return train_dataset, dev_dataset,test_dataset, [train_data_packet,dev_data_packet,test_data_packet]\n",
    "\n",
    "\n",
    "\n",
    "def model_init(\n",
    "    vocab_size,\n",
    "    context_delimiter_id,\n",
    "    model_base=\"facebook/bart-base\",\n",
    "    use_random_restriction=False,\n",
    "    section_prob=(0.25, 0.45),\n",
    "    device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n",
    "):\n",
    "    def build_model():\n",
    "        bart_config = BartConfig.from_pretrained(model_base)\n",
    "        bart_config.context_delimiter_id = context_delimiter_id\n",
    "        bart_config.use_random_restriction = use_random_restriction\n",
    "        bart_config.section_prob = section_prob\n",
    "\n",
    "        generator = BartForContextualRecovery.from_pretrained(\n",
    "            model_base, config=bart_config, ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "        # update the tokens\n",
    "        generator.resize_token_embeddings(vocab_size)  # type: ignore\n",
    "        return generator.to(device)  # type: ignore\n",
    "    return build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing files:  ['summarisation_data/xsum_train.csv']\n",
      "processing files:  ['summarisation_data/xsum_dev.csv']\n",
      "processing files:  ['summarisation_data/xsum_test.csv']\n",
      "Training Data size: 162548\n",
      "Training Data size: 9049\n",
      "The model will be trained as a non auto-encoder\n",
      "The model will be trained as a non auto-encoder\n",
      "The model will be trained as a non auto-encoder\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Args:\n",
    "    model_base: str\n",
    "    sep_token: str = \"[SEP]\"\n",
    "    is_not_auto_encoder_data: bool = True\n",
    "    \n",
    "    \n",
    "args = Args(model_base=\"facebook/bart-base\")\n",
    "train_dataset, dev_dataset,test_dataset, [train_data_packet,dev_data_packet,test_data_packet] = generate_tokenizer_and_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_delimiter_id = train_dataset.tokenizer.get_vocab()['[SEP]']\n",
    "\n",
    "train_model_path = \"trained_models_sum/bart_base_model_full_e10/checkpoint-130040/pytorch_model.bin\"\n",
    "#\"trained_models_mtl/bart_base_model_full/checkpoint-263195/pytorch_model.bin\"\n",
    "\n",
    "generator = model_init(len(train_dataset.tokenizer),\n",
    "                       context_delimiter_id=context_delimiter_id,\n",
    "                       model_base=args.model_base,use_random_restriction=False)()\n",
    "\n",
    "state_dict = torch.load(train_model_path)\n",
    "generator.load_state_dict(state_dict)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be trained as a non auto-encoder\n"
     ]
    }
   ],
   "source": [
    "dataset = ContextGenerationDataset(test_dataset.tokenizer,\n",
    "                                   nb_records=1,\n",
    "                                   section_boundary=(0.4,0.48),\n",
    "                                   \n",
    "        context_seperator=args.sep_token,\n",
    "        is_auto_encoder_data=not args.is_not_auto_encoder_data,\n",
    "                                   use_random_restrictive=True)\n",
    "dataset.change_data_mode(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, torch.Size([1, 159]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataset_processor import ContextualGenerationData\n",
    "from pytorch_lightning import seed_everything\n",
    "data = ContextualGenerationData(input=\"\"\"\n",
    "                                We are helping the community work together towards the goal of advancing Machine Learning üî•.\n",
    "The Hugging Face Hub is a platform with over 60K models, 6K datasets, and 6K demos in which people can easily collaborate in their ML workflows. \n",
    "The Hub works as a central place where anyone can share, explore, discover, and experiment with open-source Machine Learning.\n",
    " No single company, including the Tech Titans, will be able to ‚Äúsolve AI‚Äù by themselves - the only way we'll achieve this is by sharing knowledge and resources in a community-centric approach. We are building the largest open-source collection of models, datasets, demos and metrics on the Hugging Face Hub to democratize and advance ML for everyone üöÄ.\n",
    "                                \"\"\".replace(\"\\n\",\"\").strip(),output=\"\")\n",
    "kk= 45\n",
    "batch = dataset.procesTexts(data)\n",
    "b_input_ids = batch.input_ids.view(1, -1).to(device)\n",
    "b_input_mask = batch.attention_mask.view(1, -1).to(device)\n",
    "batch.section_point, b_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Calls have been made for a room in Wrexham where heroin users can inject safely under supervision.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_packet[kk].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Work to rebuild a shopping centre car park that has been beset with structural problems for a decade will now go ahead in the spring of 2015.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_packet[0].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 905/905 [07:42<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,SequentialSampler\n",
    "import tqdm\n",
    "test_data_loader = DataLoader(test_dataset,batch_size=10,\n",
    "                              sampler= SequentialSampler(test_dataset),\n",
    "                              collate_fn= SmartCollator(\n",
    "            pad_token_id=train_dataset.tokenizer.pad_token_id, max_len=700\n",
    "        )\n",
    "                              )\n",
    "\n",
    "output_summaries2 =[]\n",
    "for batch in tqdm.tqdm(test_data_loader):\n",
    "    b_input_ids = batch['input_ids'].to(device)\n",
    "    b_input_mask = batch['attention_mask'].to(device)\n",
    "    bb=generator.generate(input_ids=b_input_ids,\n",
    "            attention_mask=b_input_mask,\n",
    "            num_beams=10,\n",
    "            do_sample=False,\n",
    "            num_return_sequences=1,\n",
    "            max_new_tokens=320)\n",
    "    \n",
    "    sentences = test_dataset.tokenizer.batch_decode(bb,\n",
    "                                                    clean_up_tokenization_spaces=True,\n",
    "                                                    skip_special_tokens=True)\n",
    "    output_summaries2+=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import writeToFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "writeToFile(output_summaries, \"summarisation_data/first_response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [c.output for c in test_data_packet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "metrics = evaluate.combine(['bleu','meteor',\"rouge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = metrics.compute(predictions=output_summaries,references=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = metrics.compute(predictions=output_summaries2,references=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.10111123997762408,\n",
       " 'precisions': [0.4275391817626705,\n",
       "  0.16091219231231763,\n",
       "  0.08245325455705126,\n",
       "  0.04565501762010622],\n",
       " 'brevity_penalty': 0.7970473802449527,\n",
       " 'length_ratio': 0.8151014472952267,\n",
       " 'translation_length': 167042,\n",
       " 'reference_length': 204934,\n",
       " 'meteor': 0.31631476431431915,\n",
       " 'rouge1': 0.37525984722540895,\n",
       " 'rouge2': 0.16023600352983652,\n",
       " 'rougeL': 0.3075186462853351,\n",
       " 'rougeLsum': 0.30746783847304143}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.11396363055865384,\n",
       " 'precisions': [0.4390870861156681,\n",
       "  0.1726855491963649,\n",
       "  0.0905894799313628,\n",
       "  0.05122320492707356],\n",
       " 'brevity_penalty': 0.8321056722435886,\n",
       " 'length_ratio': 0.8447402578391092,\n",
       " 'translation_length': 173116,\n",
       " 'reference_length': 204934,\n",
       " 'meteor': 0.338673154674504,\n",
       " 'rouge1': 0.39394898657436117,\n",
       " 'rouge2': 0.1743277087302274,\n",
       " 'rougeL': 0.3234069211449011,\n",
       " 'rougeLsum': 0.3235298412651805}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Hugging Face Hub, the world's largest open-source machine learning platform, has been launched.\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb=generator.generate(input_ids=b_input_ids,\n",
    "            attention_mask=b_input_mask,\n",
    "            num_beams=10,\n",
    "            do_sample=False,\n",
    "            num_return_sequences=1,\n",
    "            max_new_tokens=320)\n",
    "test_dataset.tokenizer.batch_decode(bb,clean_up_tokenization_spaces=True,skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "473435c5caf2da67d3d84349b3ab99ae605588908510e1f3cdf041055f6c21f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
