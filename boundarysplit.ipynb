{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from functools import partial\n",
    "import nltk\n",
    "from src.contextual_bart import ContextualisedBartModel,BartForContextualRecovery,SimplifiedBeamSearch\n",
    "from src.dataset_processor import load_all_data\n",
    "from src.utils import SmartCollator, get_args, setuptokenizer\n",
    "from src.dataset_processor import (\n",
    "    ContextGenerationDataset,\n",
    ")\n",
    "from transformers import BartTokenizer, BartConfig,BartForConditionalGeneration\n",
    "from src.model_utils import CustomTrainer, get_training_arguments\n",
    "import torch\n",
    "from src.config import DATASET_PATH\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "import pickle as pk\n",
    "import torch\n",
    "from transformers import (    AutoTokenizer,\n",
    "          AutoModelForSeq2SeqLM,\n",
    "         LogitsProcessorList,    MinLengthLogitsProcessor, StoppingCriteriaList, MaxLengthCriteria,\n",
    "         TopKLogitsWarper, TemperatureLogitsWarper,BeamSearchScorer,)\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "DATASET_PATH = \"summarisation_data2/\"\n",
    "\n",
    "def generate_data():\n",
    "\n",
    "    # load the dataset\n",
    "\n",
    "    train_data_packet = load_all_data(DATASET_PATH, mode=\"train\")\n",
    "    dev_data_packet = load_all_data(DATASET_PATH, mode=\"dev\")\n",
    "    test_data_packet = load_all_data(DATASET_PATH, mode=\"test\")\n",
    "\n",
    "    print(f\"Training Data-size: {len(train_data_packet)}\")\n",
    "    print(f\"Test Data-size: {len(test_data_packet)}\")\n",
    "    return train_data_packet,dev_data_packet,test_data_packet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing files:  ['summarisation_data2/xsum_train.csv']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_data_packet,dev_data_packet,test_data_packet \u001b[39m=\u001b[39m generate_data()\n",
      "\u001b[1;32m/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb Cell 2\u001b[0m in \u001b[0;36mgenerate_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_data\u001b[39m():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# load the dataset\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     train_data_packet \u001b[39m=\u001b[39m load_all_data(DATASET_PATH, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     dev_data_packet \u001b[39m=\u001b[39m load_all_data(DATASET_PATH, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdev\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     test_data_packet \u001b[39m=\u001b[39m load_all_data(DATASET_PATH, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/src/dataset_processor.py:64\u001b[0m, in \u001b[0;36mload_all_data\u001b[0;34m(dataset_path, mode, new_format_data)\u001b[0m\n\u001b[1;32m     62\u001b[0m dataset \u001b[39m=\u001b[39m []\n\u001b[1;32m     63\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m files:\n\u001b[0;32m---> 64\u001b[0m     dataset \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m load_dataset(file,new_dataFormat\u001b[39m=\u001b[39;49mnew_format_data)\n\u001b[1;32m     65\u001b[0m random\u001b[39m.\u001b[39mshuffle(dataset)\n\u001b[1;32m     66\u001b[0m random\u001b[39m.\u001b[39mshuffle(dataset)\n",
      "File \u001b[0;32m/media/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/src/dataset_processor.py:47\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(data_path, new_dataFormat)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_dataset\u001b[39m(data_path: \u001b[39mstr\u001b[39m,new_dataFormat\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m new_dataFormat:\n\u001b[0;32m---> 47\u001b[0m         pack \u001b[39m=\u001b[39m  read_csv(data_path)\n\u001b[1;32m     48\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         pack \u001b[39m=\u001b[39m read_csv2(data_path)\n",
      "File \u001b[0;32m/media/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/src/dataset_processor.py:22\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     20\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file_name)\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     21\u001b[0m pack\u001b[39m=\u001b[39m[]\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfor\u001b[39;00m idx,document,summary \u001b[39min\u001b[39;00m data:\n\u001b[1;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mstr\u001b[39m(document)\u001b[39m.\u001b[39msplit()) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m     24\u001b[0m         pack\u001b[39m.\u001b[39mappend(ContextualGenerationData(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(document),output\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(summary)))\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "train_data_packet,dev_data_packet,test_data_packet = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextualGenerationData(input='Walls, hoardings, shop shutters and camper vans will be painted live by artists from South America and Europe throughout the weekend. Upfest, which began in 2008, is centred on Bedminster\\'s North Street. Some murals will remain until the next Upfest, including huge paintings on the Tobacco Factory venue and the Red Point climbing centre. Visitors to the festival were able to watch 275 artists from 25 countries painting during live shows over the weekend. Among those taking part are Inkie, My Dog Sighs, Gamma, Lonac and Dutch due Telmo Miel. As well as walls and vehicles, Upfest has its own 14m (45ft) long version of a New York-style subway train, created in 2012, for artists to decorate. At the Tobacco Factory Argentinian artist Martin Ron - known for huge surreal paintings in Buenos Aires - was painting a 2,025 sq ft (188 sq m) mural of his girlfriend. Bristol, the birthplace of Banksy, has an international reputation for its street art. Gallery owner Stephen Hayles founded Upfest in 2008. Initially held indoors, the next year it spilled out onto the streets with artists painting shutters, sides and end gables of shops and other buildings. Many of the murals are painted on temporary hoardings but some remain year round and become local landmarks. Mr Hayles said since Upfest began he has only had to remove two murals where artists \"overstepped the mark\". \"There was one opposite a school which was showing a bit too much cleavage,\" he said. \"It\\'s just common sense really.\"', output='Hundreds of international artists have been in Bristol to take part in an \"urban paint festival\".', boundary=-1, focus_txt='', boundary_proportion=(0.5, 100))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_packet[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from src.dataset_processor import ContextualGenerationData\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from dataclasses import asdict\n",
    "def boundarySplit(data: ContextualGenerationData,\n",
    "                  boundary_limits = (0.25, 0.8),\n",
    "                  max_context_tokens=[50, 200],\n",
    "                  nb_examples= 3):\n",
    "    data_pack=[]\n",
    "    max_context = np.arange(max_context_tokens[0], max_context_tokens[-1], step=5)\n",
    "    random.shuffle(max_context)\n",
    "    for idx  in range(nb_examples):\n",
    "        boundary_portion = np.round(np.random.uniform(\n",
    "                    size=(1,),\n",
    "                    low=boundary_limits[0],\n",
    "                    high=boundary_limits[1],\n",
    "                )[0],2)\n",
    "        d = copy.deepcopy(data)\n",
    "        d.boundary_proportion = (boundary_portion,int(max_context[idx]))\n",
    "        data_pack.append(asdict(d))\n",
    "    return data_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = [boundarySplit(data) for data in train_data_packet]\n",
    "new_test_data =  [boundarySplit(data) for data in test_data_packet]\n",
    "new_validation_data = [boundarySplit(data) for data in dev_data_packet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def getdataframe(pack):\n",
    "    data_pack = pd.DataFrame(pack)\n",
    "    #data_pack= data_pack.drop_duplicates(subset = [\"output\"],keep=\"last\")\n",
    "    return data_pack\n",
    "new_train_data = getdataframe(list(itertools.chain(*new_train_data)))\n",
    "new_test_data = getdataframe(list(itertools.chain(*new_test_data)))\n",
    "new_validation_data = getdataframe(list(itertools.chain(*new_validation_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203083, 609249)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_packet),len(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"summarisation_data2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data.to_csv('summarisation_data2/xsum_train.csv')\n",
    "new_test_data.to_csv('summarisation_data2/xsum_dev.csv')\n",
    "new_validation_data.to_csv('summarisation_data2/xsum_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv2(file_name):\n",
    "    data = pd.read_csv(file_name).values\n",
    "    pack=[]\n",
    "    for idx,document,summary,_,_,boundary in data:\n",
    "        if len(str(document).split()) > 2:\n",
    "            pack.append(ContextualGenerationData(input=str(document),\n",
    "                                                 boundary_proportion=boundary,output=str(summary)))\n",
    "    return pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg= read_csv2('summarisation_data2/xsum_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eval(gg[0].boundary_proportion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = setuptokenizer(\n",
    "        model_base=\"facebook/bart-base\", special_tokens=[], additional_tokens=['[SEP]']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz= gg[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_pack = tokenizer(\n",
    "            zz.input,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "passage_seq = passage_pack[\"input_ids\"].flatten()\n",
    "passage_attention = passage_pack[\"attention_mask\"].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([147])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_position = round(passage_seq.shape[0]*eval(zz.boundary_proportion)[0])\n",
    "end_position = start_position+ eval(zz.boundary_proportion)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283, 338, 545)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_position,end_position,passage_seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34, 200)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.boundary_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15268,   583, 14961,    23,   601,    35,   541,  5050,    15,   307,\n",
       "            4,  1868,   604,    58,   551,     7,  1098,     6,    19,     5,\n",
       "         7801,  1001,   225,  1393,    26,     7,    28,  2008,     4,   522,\n",
       "           26,    10,  2182,  4408,    11,     5, 22264,   962,    11,  1098,\n",
       "           15,   273,     4,   264,    56,    57,   551,     7,  3929,  4690,\n",
       "         2392,     6,    11,  8353,   111,   147,     5,  7801,  1001,   225,\n",
       "         1393,    16,   145,  3032,   111,    71,     5,  3213,     6,    95,\n",
       "           81,     5,  1424,    31,     5,  1398,  1891,  9959,  1139,     9,\n",
       "         1745,  1054,     4,    20,  1370,    26,    69,   220,     9, 24438,\n",
       "            8,     5, 16489,    33,    57,  3978,     4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage_seq[start_position:end_position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple\n",
    "from src.model_utils import Features\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ContextWithBoundaryFeatures:\n",
    "    input_ids: List[int]\n",
    "    attention_mask: List[int]\n",
    "    labels: Optional[List[int]]\n",
    "    decoder_attention_mask: Optional[List[int]]\n",
    "    boundary: Optional[Tuple[int,int]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ContextGenerationDatasetBoundary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m max_seq_len\u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m is_not_auto_encoder_data\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m ContextGenerationDatasetBoundary(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         nb_records\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_data_packet),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         max_len\u001b[39m=\u001b[39mmax_seq_len,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         context_seperator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[SEP]\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         is_auto_encoder_data\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m is_not_auto_encoder_data,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         use_special_token\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m train_dataset\u001b[39m.\u001b[39mchange_data_mode(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/contextualised_sentence_embedding/boundarysplit.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m train_dataset\u001b[39m.\u001b[39mset_record(train_data_packet)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ContextGenerationDatasetBoundary' is not defined"
     ]
    }
   ],
   "source": [
    "model_base = \"facebook/bart-base\"\n",
    "tokenizer = setuptokenizer(\n",
    "        model_base=model_base, special_tokens=[], additional_tokens=[\"[SEP]\"]\n",
    "    )\n",
    "    # tokenizer.add_tokens([])\n",
    "max_seq_len= 512\n",
    "is_not_auto_encoder_data=True\n",
    "train_dataset = ContextGenerationDatasetBoundary(\n",
    "        tokenizer=tokenizer,\n",
    "        nb_records=len(train_data_packet),\n",
    "        max_len=max_seq_len,\n",
    "        context_seperator=\"[SEP]\",\n",
    "        is_auto_encoder_data=not is_not_auto_encoder_data,\n",
    "        use_special_token=True,\n",
    "    )\n",
    "train_dataset.change_data_mode(1)\n",
    "train_dataset.set_record(train_data_packet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from logging import Logger\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    ")\n",
    "from transformers.models.bart.modeling_bart import (\n",
    "    BartConfig,\n",
    "    BartDecoder,\n",
    "    BartEncoderLayer,\n",
    "    BartLearnedPositionalEmbedding,\n",
    "    BartPretrainedModel,\n",
    "    BaseModelOutput,\n",
    "    CrossEntropyLoss,\n",
    "    Seq2SeqLMOutput,\n",
    "    Seq2SeqModelOutput,\n",
    "    _expand_mask,\n",
    "    shift_tokens_right,\n",
    ")\n",
    "from src.model_utils import EncoderOutputs\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "yuy = torch.LongTensor([(23,100),(56,198)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=yuy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.contextual_bart import BartEncoderBoundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-base were not used when initializing BartEncoderBoundary: ['encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'encoder.layers.0.final_layer_norm.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.weight', 'decoder.layernorm_embedding.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.0.fc1.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.fc1.bias', 'encoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'encoder.layers.4.fc2.bias', 'encoder.embed_positions.weight', 'encoder.layers.1.fc2.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'encoder.layers.0.fc2.weight', 'encoder.layers.1.fc1.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.embed_positions.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc1.weight', 'decoder.layers.5.encoder_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.2.fc1.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'decoder.embed_tokens.weight', 'decoder.layernorm_embedding.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.1.fc2.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'encoder.layers.3.fc1.bias', 'encoder.layers.4.final_layer_norm.bias', 'decoder.layers.3.fc2.bias', 'decoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.1.encoder_attn.q_proj.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.5.fc1.bias', 'decoder.layers.1.fc1.bias', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.2.self_attn_layer_norm.bias', 'shared.weight', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.5.fc2.bias', 'encoder.layers.3.fc2.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.4.fc1.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.3.final_layer_norm.bias', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.0.final_layer_norm.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'encoder.layers.2.final_layer_norm.bias', 'decoder.layers.4.fc2.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'encoder.layers.0.fc2.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.0.fc1.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.bias', 'encoder.layers.5.fc2.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'encoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.2.self_attn.q_proj.bias', 'encoder.layernorm_embedding.bias', 'encoder.layers.3.fc1.weight', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.fc1.weight', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.final_layer_norm.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'encoder.embed_tokens.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.final_layer_norm.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.3.fc2.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.1.fc2.weight', 'decoder.layers.2.encoder_attn.out_proj.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.0.fc2.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.5.fc1.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.1.fc1.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.1.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layernorm_embedding.weight', 'encoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.v_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.5.final_layer_norm.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'encoder.layers.5.final_layer_norm.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.bias']\n",
      "- This IS expected if you are initializing BartEncoderBoundary from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartEncoderBoundary from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartEncoderBoundary were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['layers.1.fc2.weight', 'layers.4.self_attn.out_proj.weight', 'layers.5.self_attn.out_proj.weight', 'layernorm_embedding.weight', 'layers.2.self_attn_layer_norm.bias', 'layers.4.fc2.bias', 'layers.1.final_layer_norm.weight', 'layers.3.final_layer_norm.weight', 'layers.1.fc2.bias', 'layers.3.self_attn.out_proj.bias', 'layers.4.self_attn.k_proj.bias', 'embed_tokens.weight', 'layers.5.fc2.bias', 'layers.4.self_attn.k_proj.weight', 'layers.5.self_attn.k_proj.bias', 'layers.1.self_attn.q_proj.bias', 'layers.2.self_attn_layer_norm.weight', 'layers.0.fc2.weight', 'layers.1.fc1.weight', 'layers.0.self_attn_layer_norm.bias', 'layers.0.self_attn.q_proj.bias', 'layers.2.final_layer_norm.bias', 'layers.4.self_attn_layer_norm.bias', 'layers.3.self_attn.k_proj.weight', 'layers.0.fc1.bias', 'layers.1.self_attn_layer_norm.bias', 'layers.3.self_attn.k_proj.bias', 'layers.3.self_attn_layer_norm.bias', 'layers.1.self_attn.v_proj.bias', 'layers.3.self_attn.q_proj.weight', 'layers.1.self_attn_layer_norm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.0.self_attn.k_proj.bias', 'layers.3.self_attn.q_proj.bias', 'layers.3.final_layer_norm.bias', 'layers.3.fc2.weight', 'layers.0.self_attn.out_proj.bias', 'layers.4.self_attn.out_proj.bias', 'layers.5.self_attn_layer_norm.bias', 'layers.5.fc1.weight', 'layers.5.self_attn.v_proj.bias', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.fc1.bias', 'layers.1.self_attn.k_proj.weight', 'layers.2.fc1.weight', 'layers.0.final_layer_norm.weight', 'layers.0.self_attn.q_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.4.fc1.bias', 'layers.0.self_attn.v_proj.weight', 'layers.3.self_attn.v_proj.bias', 'layers.2.self_attn.k_proj.bias', 'layers.1.self_attn.out_proj.bias', 'layers.2.fc2.weight', 'layers.2.final_layer_norm.weight', 'layers.3.self_attn.out_proj.weight', 'layers.5.fc1.bias', 'layers.2.self_attn.out_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.4.fc2.weight', 'layers.3.fc1.bias', 'layers.3.self_attn_layer_norm.weight', 'layers.5.final_layer_norm.bias', 'layernorm_embedding.bias', 'layers.0.self_attn.v_proj.bias', 'layers.3.fc2.bias', 'layers.0.self_attn_layer_norm.weight', 'layers.4.self_attn.v_proj.bias', 'layers.2.fc2.bias', 'layers.4.fc1.weight', 'layers.2.self_attn.out_proj.bias', 'layers.0.self_attn.out_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.3.fc1.weight', 'layers.5.fc2.weight', 'layers.1.self_attn.out_proj.weight', 'layers.4.self_attn_layer_norm.weight', 'layers.4.final_layer_norm.weight', 'layers.4.self_attn.v_proj.weight', 'layers.2.self_attn.v_proj.bias', 'layers.5.final_layer_norm.weight', 'layers.0.fc2.bias', 'layers.5.self_attn.q_proj.bias', 'layers.0.self_attn.k_proj.weight', 'layers.0.fc1.weight', 'layers.2.self_attn.q_proj.bias', 'layers.1.self_attn.v_proj.weight', 'embed_positions.weight', 'layers.5.self_attn.out_proj.bias', 'layers.1.self_attn.q_proj.weight', 'layers.4.self_attn.q_proj.bias', 'layers.4.final_layer_norm.bias', 'layers.1.final_layer_norm.bias', 'layers.5.self_attn_layer_norm.weight', 'layers.1.self_attn.k_proj.bias', 'layers.0.final_layer_norm.bias', 'layers.1.fc1.bias', 'layers.2.self_attn.v_proj.weight', 'layers.4.self_attn.q_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "encoder = BartEncoderBoundary.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = torch.LongTensor([(start_position,end_position),]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eout = encoder(passage_seq.view(1,-1),passage_attention.view(1,-1), boundaries = boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 55])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eout.cleaned_mask.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "473435c5caf2da67d3d84349b3ab99ae605588908510e1f3cdf041055f6c21f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
