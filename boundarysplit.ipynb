{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from functools import partial\n",
    "import nltk\n",
    "from src.contextual_bart import ContextualisedBartModel,BartForContextualRecovery,SimplifiedBeamSearch\n",
    "from src.dataset_processor import load_all_data\n",
    "from src.utils import SmartCollator, get_args, setuptokenizer\n",
    "from src.dataset_processor import (\n",
    "    ContextGenerationDataset,\n",
    ")\n",
    "from transformers import BartTokenizer, BartConfig,BartForConditionalGeneration\n",
    "from src.model_utils import CustomTrainer, get_training_arguments\n",
    "import torch\n",
    "from src.config import DATASET_PATH\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "import pickle as pk\n",
    "import torch\n",
    "from transformers import (    AutoTokenizer,\n",
    "          AutoModelForSeq2SeqLM,\n",
    "         LogitsProcessorList,    MinLengthLogitsProcessor, StoppingCriteriaList, MaxLengthCriteria,\n",
    "         TopKLogitsWarper, TemperatureLogitsWarper,BeamSearchScorer,)\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "DATASET_PATH = \"summarisation_data/\"\n",
    "\n",
    "def generate_data():\n",
    "\n",
    "    # load the dataset\n",
    "\n",
    "    train_data_packet = load_all_data(DATASET_PATH, mode=\"train\")\n",
    "    dev_data_packet = load_all_data(DATASET_PATH, mode=\"dev\")\n",
    "    test_data_packet = load_all_data(DATASET_PATH,mode=\"test\")\n",
    "\n",
    "    print(f\"Training Data size: {len(train_data_packet)}\")\n",
    "    print(f\"Training Data size: {len(test_data_packet)}\")\n",
    "    return train_data_packet,dev_data_packet,test_data_packet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing files:  ['summarisation_data/xsum_train.csv']\n",
      "processing files:  ['summarisation_data/xsum_dev.csv']\n",
      "processing files:  ['summarisation_data/xsum_test.csv']\n",
      "Training Data size: 203083\n",
      "Training Data size: 11322\n"
     ]
    }
   ],
   "source": [
    "train_data_packet,dev_data_packet,test_data_packet = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from src.dataset_processor import ContextualGenerationData\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "def boundarySplit(data: ContextualGenerationData,\n",
    "                  boundary_limits = (0.25,\n",
    "                                     0.8),\n",
    "                  max_context_tokens=[50,200],\n",
    "                  nb_examples= 3\n",
    "                  ):\n",
    "    data_pack=[]\n",
    "    max_context = np.linspace(max_context_tokens[0],max_context_tokens[-1],num=4)\n",
    "    random.shuffle(max_context)\n",
    "    for idx  in range(nb_examples):\n",
    "        \n",
    "        boundary_portion = np.round(np.random.uniform(\n",
    "                    size=(1,),\n",
    "                    low=boundary_limits[0],\n",
    "                    high=boundary_limits[1],\n",
    "                )[0],2)\n",
    "        d= copy.deepcopy(data)\n",
    "        d.boundary_proportion = (boundary_portion,int(max_context[idx]))\n",
    "        data_pack.append(d)\n",
    "    return data_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff= boundarySplit(train_data_packet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextualGenerationData(input='A Citroen C5, driven by a man, 80, from Stratford-upon-Avon, Warwickshire, and a Jaguar X-Type, driven by a man, 81, of Llandrindod Wells, collided near Walton at 17:30 GMT on Wednesday. Both men were taken to hospital, with the Citroen driver said to be critical. Police said a female passenger in the Jaguar died in hospital on Friday. She had been taken to Queen Elizabeth Hospital, in Birmingham - where the Citroen driver is being treated - after the accident, just over the border from the Herefordshire town of Kington. The force said her next of kin and the coroner have been informed.', output='A woman has died after a collision involving two cars on the A44 in Powys, Dyfed-Powys Police has said.', boundary=-1, focus_txt='', boundary_proportion=(0.79, 50))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "473435c5caf2da67d3d84349b3ab99ae605588908510e1f3cdf041055f6c21f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
