{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartTokenizer,BartConfig\n",
    "tokeniser = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "tokeniser.add_tokens(['[SEP]'])\n",
    "tokeniser.get_added_vocab()['[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.contextual_bart import ContextualisedBartModel,BartForContextualRecovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_config = BartConfig.from_pretrained('facebook/bart-base')\n",
    "bart_config.context_delimiter_id = tokeniser.get_added_vocab()['[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_bart = BartForContextualRecovery.from_pretrained('facebook/bart-base',\n",
    "                                                   config = bart_config,\n",
    "                                                   ignore_mismatched_sizes=True\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50266, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_bart.resize_token_embeddings(len(tokeniser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderOutputs(last_hidden_state=tensor([[[ 0.0504,  0.0618,  0.3333,  ...,  0.1143, -0.0497,  0.1502],\n",
      "         [-0.1628, -0.2547,  0.1759,  ...,  0.0351,  0.3040,  0.1871],\n",
      "         [-0.0157,  0.1318,  0.1969,  ...,  0.1808, -0.1956,  0.2334],\n",
      "         [-0.1582, -0.0009,  0.2791,  ...,  0.1717, -0.1716,  0.4238],\n",
      "         [ 0.0728,  0.1454,  0.1331,  ...,  0.1525, -0.0421,  0.0894]],\n",
      "\n",
      "        [[-0.0839,  0.0606,  0.1464,  ...,  0.2486,  0.1477,  0.1590],\n",
      "         [ 0.0529,  0.1785, -0.1232,  ...,  0.1522, -0.0398,  0.1828],\n",
      "         [-0.1257, -0.0641,  0.5198,  ...,  0.2859, -0.2460,  0.5156],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<CatBackward0>), hidden_states=None, attentions=None, attention_mask=tensor([[1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 0.]]))\n",
      "torch.Size([2, 5, 768])  huye\n",
      "torch.Size([2, 5])  Meeu\n"
     ]
    }
   ],
   "source": [
    "oop_tok = tokeniser(['He was here [SEP] boy friend.',\n",
    "                     \"We can find him because [SEP] he was\"],\n",
    "                    return_tensors=\"pt\",padding=True)\n",
    "oop = con_bart(**oop_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset_processor import ContextGenerationDataset, load_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_all_data('processed_data/',)\n",
    "dev_data = load_all_data('processed_data/','dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dat in train_data:\n",
    "    if '[SEP]' not in dat.input:\n",
    "        print(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ContextGenerationDataset(tokeniser,)\n",
    "train_dataset.set_record(train_data)\n",
    "\n",
    "dev_dataset = ContextGenerationDataset(tokeniser,)\n",
    "dev_dataset.set_record(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_delimiter_id = tokeniser.get_added_vocab()['[SEP]']\n",
    "def _strip_context(input_ids, ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_ids:\n",
    "        :param embeddings:\n",
    "        :param attention_mask:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # identify the locations of the context_delimiter in each of the input sequence\n",
    "        if type(input_ids) is list:\n",
    "            input_ids = torch.LongTensor(\n",
    "                input_ids,\n",
    "            )\n",
    "        delimiter_points = input_ids == context_delimiter_id\n",
    "        delimiter_points_idxs = delimiter_points.nonzero(as_tuple=True)[-1]\n",
    "        \n",
    "        if len(delimiter_points_idxs)!=input_ids.shape[0]:\n",
    "            print(delimiter_points_idxs)\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,RandomSampler,SequentialSampler\n",
    "from src.utils import SmartCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_collator=SmartCollator(pad_token_id=train_dataset.tokenizer.pad_token_id, \n",
    "                                    max_len=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(train_dataset,batch_size=16,\n",
    "                         sampler=SequentialSampler(train_dataset),\n",
    "                         collate_fn=smart_collator\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_delimiter_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser.decode(dat['input_ids'][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[idx for idx,d in enumerate(dat['input_ids']) if 50265 not in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,dat in enumerate(data_loader):\n",
    "    if not _strip_context(dat['input_ids']):\n",
    "        for v,d in enumerate(dat):\n",
    "            _strip_context(d)\n",
    "        print(idx)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "473435c5caf2da67d3d84349b3ab99ae605588908510e1f3cdf041055f6c21f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
