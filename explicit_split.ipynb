{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from functools import partial\n",
    "import nltk\n",
    "from src.contextual_bart import ContextualisedBartModel,BartForContextualRecovery,SimplifiedBeamSearch\n",
    "from src.dataset_processor import load_all_data\n",
    "from src.utils import SmartCollator, get_args, setuptokenizer\n",
    "from src.dataset_processor import (\n",
    "    ContextGenerationDataset,\n",
    ")\n",
    "from transformers import BartTokenizer, BartConfig,BartForConditionalGeneration\n",
    "from src.model_utils import CustomTrainer, get_training_arguments\n",
    "import torch\n",
    "from src.config import DATASET_PATH\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "import pickle as pk\n",
    "import torch\n",
    "from transformers import (    AutoTokenizer,\n",
    "          AutoModelForSeq2SeqLM,\n",
    "         LogitsProcessorList,    MinLengthLogitsProcessor, StoppingCriteriaList, MaxLengthCriteria,\n",
    "         TopKLogitsWarper, TemperatureLogitsWarper,BeamSearchScorer,)\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "DATASET_PATH = \"summarisation_data/\"\n",
    "\n",
    "def generate_data():\n",
    "\n",
    "    # load the dataset\n",
    "\n",
    "    train_data_packet = load_all_data(DATASET_PATH, mode=\"train\")\n",
    "    dev_data_packet = load_all_data(DATASET_PATH, mode=\"dev\")\n",
    "    test_data_packet = load_all_data(DATASET_PATH,mode=\"test\")\n",
    "\n",
    "    print(f\"Training Data size: {len(train_data_packet)}\")\n",
    "    print(f\"Training Data size: {len(test_data_packet)}\")\n",
    "    return train_data_packet,dev_data_packet,test_data_packet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing files:  ['summarisation_data/xsum_train.csv']\n",
      "processing files:  ['summarisation_data/xsum_dev.csv']\n",
      "processing files:  ['summarisation_data/xsum_test.csv']\n",
      "Training Data size: 203083\n",
      "Training Data size: 11322\n"
     ]
    }
   ],
   "source": [
    "train_data_packet,dev_data_packet,test_data_packet = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextualGenerationData(input='McGeady will be reunited with new Black Cats boss Simon Grayson, for whom he played 35 games, scoring eight times, at Preston North End last season. The 31-year-old, who began his career at Celtic, joined Everton from Spartak Moscow in January 2014 and subsequently played 41 games, scoring once. He has also made 90 appearances for the Republic of Ireland. Find all the latest football transfers on our dedicated page.', output='Everton winger Aiden McGeady is to join Championship side Sunderland on a permanent deal.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_packet[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_boundary = (0.7,0.9)\n",
    "moderate_boundary  = (0.45,0.7)\n",
    "simple_boundary  = (0.2,0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from src.dataset_processor import ContextualGenerationData\n",
    "import numpy as np\n",
    "def extendData(data: ContextualGenerationData):\n",
    "    def get_section_bounday(section_boundary,nb_words):\n",
    "        section_point = round(\n",
    "            (\n",
    "                np.random.uniform(\n",
    "                    size=(1,),\n",
    "                    low=section_boundary[0],\n",
    "                    high=section_boundary[1],\n",
    "                )\n",
    "                * nb_words\n",
    "            )[0]\n",
    "        )\n",
    "        return section_point\n",
    "    \n",
    "    passage = data.input\n",
    "    clean_passage = \" \".join(passage.replace(\"[SEP]\", \n",
    "                                             \"\").strip().split()).strip()\n",
    "    passage_sentence_tokenized = clean_passage.strip().split()\n",
    "    nb_words = len(passage_sentence_tokenized)\n",
    "    \n",
    "    extreme_section = get_section_bounday(extreme_boundary,nb_words)\n",
    "    moderate_section = get_section_bounday(moderate_boundary,nb_words)\n",
    "    simple_section = get_section_bounday(simple_boundary,nb_words)\n",
    "    \n",
    "    print(extreme_section,moderate_section,simple_section)\n",
    "    \n",
    "    repeated_data =[copy.deepcopy(data),copy.deepcopy(data),copy.deepcopy(data)]\n",
    "    repeated_data[0].boundary = simple_section\n",
    "    repeated_data[1].boundary = moderate_section\n",
    "    repeated_data[2].boundary = extreme_section\n",
    "    \n",
    "    return repeated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 165 66\n"
     ]
    }
   ],
   "source": [
    "cc=extendData(train_data_packet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp=nltk.tokenize.sent_tokenize(cc[0].input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "def expand_data_ngram(data: ContextualGenerationData,n_count= 2):\n",
    "    input_text =  data.input\n",
    "    sentences =  nltk.tokenize.sent_tokenize(input_text)\n",
    "    \n",
    "    unigram_texts = random.sample(sentences,random.choice([2,3,3,2]))\n",
    "    \n",
    "    # create the bigram texts \n",
    "    bi_grams = random.sample(list(ngrams(sentences,2)),2)\n",
    "    \n",
    "    # trigram texts\n",
    "    tri_grams = random.sample(list(ngrams(sentences,3)),2)\n",
    "    \n",
    "    # nanogram texts\n",
    "    nano_grams = random.sample(list(ngrams(sentences,4)),random.choice([2,1,2]))\n",
    "    \n",
    "    examples = unigram_texts+bi_grams+tri_grams+nano_grams\n",
    "    \n",
    "    data_pack = []\n",
    "    for exam in examples:\n",
    "        d= copy.deepcopy(data)\n",
    "        d.focus_txt = exam\n",
    "        data_pack.append(d)\n",
    "    return data_pack\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = expand_data_ngram(train_data_packet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextualGenerationData(input='A number of Afghan troops also died in the shooting at a remote military base in Wardak province, not far from Kabul. The US military called the attack a \"betrayal\". The killer - a member of the Afghan security forces - was shot dead at the scene. Last year more than 60 Nato troops were killed by Afghan security personnel or insurgents posing as them. The gunman - a member of the Afghan army or police - opened fire as US special forces and Afghan commandos held an early morning meeting. Another 10 US troops were wounded, the US military said. The number of Afghan casualties has not been made public. The attacker was also killed and the US-led special operations task force said the area had been secured. Wardak province is the scene of particular tension between the Afghan authorities and US troops fighting Taliban militants. Afghan President Hamid Karzai had ordered US special forces to leave the province by the middle of this month because of allegations of torture and disappearances carried out by Afghan troops working with them. Earlier on Monday, police in Kabul said two civilian lorry drivers were killed and one wounded when they were fired on by an Isaf convoy. The international security force Isaf said soldiers had opened fire to protect themselves when the two drivers failed to comply with a warning. The latest deaths come amid continuing tension between the Afghan government and the US, as Nato troops prepare to end combat operations next year. President Karzai has been strongly critical of US and allied forces for causing civilian casualties. So-called \"insider attacks\" by members of the Afghan security forces, or Taliban infiltrators posing as them, have accounted for a growing proportion of Isaf casualties.', output='Two US soldiers in Afghanistan have been shot dead in a so-called insider attack, US and Afghan sources say.', boundary=-1, focus_txt='The killer - a member of the Afghan security forces - was shot dead at the scene.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List\n",
    "from src.model_utils import Features\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "logger = logging.getLogger(__name__)\n",
    "class ContextGenerationDatasetPicks(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        nb_records: int = 1,\n",
    "        max_len=700,\n",
    "        use_random_restrictive: bool = False,\n",
    "        context_seperator: str = \"[SEP]\",\n",
    "        use_special_token: bool = True,\n",
    "        is_auto_encoder_data: bool = True,) -> None:\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.nb_records = nb_records\n",
    "        self.is_records_set = False\n",
    "        self.use_random_restrictive = use_random_restrictive\n",
    "        self.data: List[ContextualGenerationData] = []\n",
    "        self.context_seperator = context_seperator\n",
    "        self._context_delimiter_id = self.tokenizer.get_vocab()[self.context_seperator]\n",
    "        self.use_special_token = use_special_token\n",
    "        self._is_auto_encoder_data = is_auto_encoder_data\n",
    "\n",
    "        if self._is_auto_encoder_data:\n",
    "            logger.info(\"The model will be trained as an auto-encoder\")\n",
    "        else:\n",
    "            logger.info(\"The model will be trained as a non auto-encoder\")\n",
    "\n",
    "        # Since we will be mainly training, we will set it to 1, during inference, we will set it to 2\n",
    "        self.change_data_mode(1)\n",
    "\n",
    "    def __len__(\n",
    "        self,\n",
    "    ):\n",
    "        return self.nb_records\n",
    "\n",
    "    def set_record(self, data):\n",
    "        self.data = data\n",
    "        self.nb_records = len(self.data)\n",
    "\n",
    "    def add_record(self, row):\n",
    "        self.data.append(row)\n",
    "        self.nb_records = len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.procesTexts(self.data[index])\n",
    "\n",
    "    def change_data_mode(self, mode=1):\n",
    "        self.mode = mode > 1\n",
    "        \n",
    "    def _sent_tokenize(self,text,focus)\n",
    "\n",
    "    def procesTexts(self, data: ContextualGenerationData):\n",
    "\n",
    "        passage = data.input\n",
    "        clean_passage = \" \".join(passage.replace(\"[SEP]\", \"\").strip().split()).strip()\n",
    "        passage_sentence_tokenized = clean_passage.strip().split()\n",
    "        nb_words = len(passage_sentence_tokenized)\n",
    "        \n",
    "        section_point = data.boundary\n",
    "        if section_point<0:\n",
    "            section_point = round(\n",
    "                (\n",
    "                    np.random.uniform(\n",
    "                        size=(1,),\n",
    "                        low=self.section_boundary[0],\n",
    "                        high=self.section_boundary[1],\n",
    "                    )\n",
    "                    * nb_words\n",
    "                )[0]\n",
    "            )\n",
    "\n",
    "        composed_input = (\n",
    "            \" \".join(passage_sentence_tokenized[:section_point])\n",
    "            + f\" {self.context_seperator} \"\n",
    "            + \" \".join(passage_sentence_tokenized[section_point:])\n",
    "        )\n",
    "\n",
    "        label_text = clean_passage if self._is_auto_encoder_data else data.output\n",
    "        # apply the tokenizer to convert the texts to the appropriate input\n",
    "        if not self.mode:\n",
    "            label_pack = self.tokenizer(\n",
    "                label_text,\n",
    "                return_tensors=\"pt\",\n",
    "                # add_special_tokens=self.use_special_token\n",
    "            )\n",
    "            label_seq = label_pack[\"input_ids\"].flatten()\n",
    "            label_attention = label_pack[\"attention_mask\"].flatten()\n",
    "\n",
    "        passage_pack = self.tokenizer(\n",
    "            composed_input,\n",
    "            add_special_tokens=self.use_special_token,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        passage_seq = passage_pack[\"input_ids\"].flatten()\n",
    "        passage_attention = passage_pack[\"attention_mask\"].flatten()\n",
    "\n",
    "        num_tokens = passage_seq.shape[-1]\n",
    "\n",
    "        if num_tokens > self.max_len:\n",
    "            delimiter_points = passage_seq == self._context_delimiter_id\n",
    "            delimiter_points_idx = delimiter_points.nonzero(as_tuple=True)[-1][0]\n",
    "            if delimiter_points_idx > self.max_len:\n",
    "                passage_seq = torch.concat(\n",
    "                    [torch.Tensor([self._context_delimiter_id]).long(), passage_seq]\n",
    "                )\n",
    "                passage_attention = torch.concat(\n",
    "                    [torch.Tensor([1]).long(), passage_attention]\n",
    "                )\n",
    "\n",
    "        if not self.mode:\n",
    "            return Features(\n",
    "                input_ids=passage_seq,\n",
    "                attention_mask=passage_attention,\n",
    "                labels=label_seq,\n",
    "                decoder_attention_mask=label_attention,\n",
    "                section_point=section_point,\n",
    "            )\n",
    "        else:\n",
    "            return Features(\n",
    "                input_ids=passage_seq,\n",
    "                attention_mask=passage_attention,\n",
    "                labels=[],\n",
    "                decoder_attention_mask=[],\n",
    "                section_point=section_point,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextGenerationDatasetBoundary(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        nb_records: int = 1,\n",
    "        max_len=700,\n",
    "        section_boundary=(0.25, 0.70),\n",
    "        use_random_restrictive: bool = False,\n",
    "        context_seperator: str = \"[SEP]\",\n",
    "        use_special_token: bool = True,\n",
    "        is_auto_encoder_data: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.nb_records = nb_records\n",
    "        self.is_records_set = False\n",
    "        self.use_random_restrictive = use_random_restrictive\n",
    "        self.section_boundary = section_boundary\n",
    "        self.data: List[ContextualGenerationData] = []\n",
    "        self.context_seperator = context_seperator\n",
    "        self._context_delimiter_id = self.tokenizer.get_vocab()[self.context_seperator]\n",
    "        self.use_special_token = use_special_token\n",
    "        self._is_auto_encoder_data = is_auto_encoder_data\n",
    "\n",
    "        if self._is_auto_encoder_data:\n",
    "            logger.info(\"The model will be trained as an auto-encoder\")\n",
    "        else:\n",
    "            logger.info(\"The model will be trained as a non auto-encoder\")\n",
    "\n",
    "        # Since we will be mainly training, we will set it to 1, during inference, we will set it to 2\n",
    "        self.change_data_mode(1)\n",
    "\n",
    "    def __len__(\n",
    "        self,\n",
    "    ):\n",
    "        return self.nb_records\n",
    "\n",
    "    def set_record(self, data):\n",
    "        self.data = data\n",
    "        self.nb_records = len(self.data)\n",
    "\n",
    "    def add_record(self, row):\n",
    "        self.data.append(row)\n",
    "        self.nb_records = len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.procesTexts(self.data[index])\n",
    "\n",
    "    def change_data_mode(self, mode=1):\n",
    "        self.mode = mode > 1\n",
    "\n",
    "    def procesTexts(self, data: ContextualGenerationData):\n",
    "\n",
    "        passage = data.input\n",
    "        clean_passage = \" \".join(passage.replace(\"[SEP]\", \"\").strip().split()).strip()\n",
    "        passage_sentence_tokenized = clean_passage.strip().split()\n",
    "        nb_words = len(passage_sentence_tokenized)\n",
    "        \n",
    "        section_point = data.boundary\n",
    "        if section_point<0:\n",
    "            section_point = round(\n",
    "                (\n",
    "                    np.random.uniform(\n",
    "                        size=(1,),\n",
    "                        low=self.section_boundary[0],\n",
    "                        high=self.section_boundary[1],\n",
    "                    )\n",
    "                    * nb_words\n",
    "                )[0]\n",
    "            )\n",
    "\n",
    "        composed_input = (\n",
    "            \" \".join(passage_sentence_tokenized[:section_point])\n",
    "            + f\" {self.context_seperator} \"\n",
    "            + \" \".join(passage_sentence_tokenized[section_point:])\n",
    "        )\n",
    "\n",
    "        label_text = clean_passage if self._is_auto_encoder_data else data.output\n",
    "        # apply the tokenizer to convert the texts to the appropriate input\n",
    "        if not self.mode:\n",
    "            label_pack = self.tokenizer(\n",
    "                label_text,\n",
    "                return_tensors=\"pt\",\n",
    "                # add_special_tokens=self.use_special_token\n",
    "            )\n",
    "            label_seq = label_pack[\"input_ids\"].flatten()\n",
    "            label_attention = label_pack[\"attention_mask\"].flatten()\n",
    "\n",
    "        passage_pack = self.tokenizer(\n",
    "            composed_input,\n",
    "            add_special_tokens=self.use_special_token,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        passage_seq = passage_pack[\"input_ids\"].flatten()\n",
    "        passage_attention = passage_pack[\"attention_mask\"].flatten()\n",
    "\n",
    "        num_tokens = passage_seq.shape[-1]\n",
    "\n",
    "        if num_tokens > self.max_len:\n",
    "            delimiter_points = passage_seq == self._context_delimiter_id\n",
    "            delimiter_points_idx = delimiter_points.nonzero(as_tuple=True)[-1][0]\n",
    "            if delimiter_points_idx > self.max_len:\n",
    "                passage_seq = torch.concat(\n",
    "                    [torch.Tensor([self._context_delimiter_id]).long(), passage_seq]\n",
    "                )\n",
    "                passage_attention = torch.concat(\n",
    "                    [torch.Tensor([1]).long(), passage_attention]\n",
    "                )\n",
    "\n",
    "        if not self.mode:\n",
    "            return Features(\n",
    "                input_ids=passage_seq,\n",
    "                attention_mask=passage_attention,\n",
    "                labels=label_seq,\n",
    "                decoder_attention_mask=label_attention,\n",
    "                section_point=section_point,\n",
    "            )\n",
    "        else:\n",
    "            return Features(\n",
    "                input_ids=passage_seq,\n",
    "                attention_mask=passage_attention,\n",
    "                labels=[],\n",
    "                decoder_attention_mask=[],\n",
    "                section_point=section_point,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "473435c5caf2da67d3d84349b3ab99ae605588908510e1f3cdf041055f6c21f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
