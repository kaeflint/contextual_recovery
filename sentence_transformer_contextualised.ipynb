{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from src.model_utils import EncoderOutputs,mean_pooling,SentenceEmbeddingOutput\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import AutoModel,AutoModelForCausalLM,PreTrainedModel,PretrainedConfig,EncoderDecoderConfig\n",
    "import torch\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "class ContextualSentenceTransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name, context_delimiter_id,\n",
    "        pad_token_id,\n",
    "        normalize = False\n",
    "    ):\n",
    "        super(ContextualSentenceTransformerEncoder, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        self._context_delimiter_id = context_delimiter_id\n",
    "        self._pad_token_id = pad_token_id\n",
    "        self.normalize_sentence_embeddings = normalize\n",
    "\n",
    "    def _strip_context(self, input_ids, embeddings, attention_mask):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_ids:\n",
    "        :param embeddings:\n",
    "        :param attention_mask:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # identify the locations of the context_delimiter in each of the input sequence\n",
    "        if type(input_ids) is list:\n",
    "            input_ids = torch.LongTensor(\n",
    "                input_ids,\n",
    "            )\n",
    "        delimiter_points = input_ids == self._context_delimiter_id\n",
    "\n",
    "        delimiter_points_idxs = delimiter_points.nonzero(as_tuple=True)[-1]\n",
    "\n",
    "        all_embeddings = []\n",
    "        all_attention_masks = []\n",
    "        all_input_ids = []\n",
    "        max_length = 0\n",
    "        embedding_dim = embeddings.shape[-1]\n",
    "\n",
    "        # For item in input_ids, embeddings, attention_mask, input_ids, select the\n",
    "        # portion of the tensor after the delimiter_point_id\n",
    "        for delimiter_point_id, embedding, att_mask in zip(\n",
    "            delimiter_points_idxs, embeddings, attention_mask\n",
    "        ):\n",
    "            embedding = embedding[delimiter_point_id + 1 :, :]\n",
    "            if max_length < embedding.shape[0]:\n",
    "                max_length = embedding.shape[0]\n",
    "            all_embeddings.append(embedding)\n",
    "            all_attention_masks.append(att_mask[delimiter_point_id + 1 :])\n",
    "\n",
    "        # Reshape all the section of interest for each item in all_input_ids, all_embeddings, all_attention_masks to\n",
    "        # the same size\n",
    "        batch_embeddings: List = list()\n",
    "        batch_attention_masks: List = list()\n",
    "\n",
    "        for idx, (embedding, att_mask) in enumerate(\n",
    "            zip(all_embeddings, all_attention_masks)\n",
    "        ):\n",
    "            len_diff = max_length - embedding.shape[0]\n",
    "            if max_length > embedding.shape[0]:\n",
    "                pad_tensor = torch.zeros(len_diff, embedding_dim).to(embedding.device)\n",
    "                embedding = torch.concat([embedding, pad_tensor], dim=0)\n",
    "\n",
    "                attn_pads = torch.zeros(\n",
    "                    len_diff,\n",
    "                ).to(att_mask.device)\n",
    "                att_mask = torch.concat([att_mask, attn_pads], -1)\n",
    "\n",
    "            batch_embeddings += [embedding.view(-1, max_length, embedding_dim)]\n",
    "            batch_attention_masks += [att_mask.view(-1, max_length)]\n",
    "\n",
    "        # Create the final tensors with the contexts removed\n",
    "        batch_attention_masks = torch.concat(batch_attention_masks, 0)\n",
    "        batch_embeddings = torch.concat(batch_embeddings, 0)\n",
    "        return batch_embeddings, batch_attention_masks\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        apply_pool: Optional[bool] = False,\n",
    "    ) -> Union[Tuple, EncoderOutputs]:\n",
    "        model_output = self.model(input_ids, attention_mask)\n",
    "        print(model_output.keys())\n",
    "\n",
    "        hidden_states, batch_encoder_attention_masks = self._strip_context(\n",
    "            input_ids,\n",
    "            model_output[\"last_hidden_state\"],\n",
    "           attention_mask,\n",
    "        )\n",
    "        encoder_states = []\n",
    "        all_attentions = []\n",
    "        \n",
    "        \n",
    "        if not return_dict:\n",
    "            \n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    encoder_states,\n",
    "                    all_attentions,\n",
    "                    batch_encoder_attention_masks,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        \n",
    "        if not apply_pool:\n",
    "            return EncoderOutputs(\n",
    "                last_hidden_state=hidden_states,\n",
    "                hidden_states=encoder_states,\n",
    "                attentions=all_attentions,\n",
    "                attention_mask=batch_encoder_attention_masks,\n",
    "                )\n",
    "        else:\n",
    "            embeddings = mean_pooling(hidden_states, \n",
    "                                      batch_encoder_attention_masks )\n",
    "            if self.normalize_sentence_embeddings:\n",
    "                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "            return SentenceEmbeddingOutput(sentence_embedding= embeddings, \n",
    "                        token_embeddings=hidden_states,\n",
    "                        attention_mask= batch_encoder_attention_masks)\n",
    "\n",
    "\n",
    "\n",
    "class ContextualSentenceTransformerModel(PreTrainedModel):\n",
    "    def __init__(self,\n",
    "                 config: Optional[PretrainedConfig] = None,\n",
    "        encoder: Optional[PreTrainedModel] = None,\n",
    "        decoder: Optional[PreTrainedModel] = None,) -> None:\n",
    "        \n",
    "        if config is None:\n",
    "            config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder.config, decoder.config)\n",
    "        else:\n",
    "            if not isinstance(config, self.config_class):\n",
    "                raise ValueError(f\"Config: {config} has to be of type {self.config_class}\")\n",
    "        \n",
    "        if config.decoder.cross_attention_hidden_size is not None:\n",
    "            if config.decoder.cross_attention_hidden_size != config.encoder.hidden_size:\n",
    "                raise ValueError(\n",
    "                    \"If `cross_attention_hidden_size` is specified in the decoder's configuration, it has to be equal\"\n",
    "                    f\" to the encoder's `hidden_size`. Got {config.decoder.cross_attention_hidden_size} for\"\n",
    "                    f\" `config.decoder.cross_attention_hidden_size` and {config.encoder.hidden_size} for\"\n",
    "                    \" `config.encoder.hidden_size`.\"\n",
    "                )\n",
    "\n",
    "        \n",
    "        # initialize with config\n",
    "        super().__init__(config)\n",
    "        \n",
    "        \n",
    "        if decoder is None:\n",
    "            decoder = AutoModelForCausalLM.from_config(config.decoder)\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        if self.encoder.config.to_dict() != self.config.encoder.to_dict():\n",
    "            logger.warning(\n",
    "                f\"Config of the encoder: {self.encoder.__class__} is overwritten by shared encoder config:\"\n",
    "                f\" {self.config.encoder}\"\n",
    "            )\n",
    "        if self.decoder.config.to_dict() != self.config.decoder.to_dict():\n",
    "            logger.warning(\n",
    "                f\"Config of the decoder: {self.decoder.__class__} is overwritten by shared decoder config:\"\n",
    "                f\" {self.config.decoder}\"\n",
    "            )\n",
    "\n",
    "        # make sure that the individual model's config refers to the shared config\n",
    "        # so that the updates to the config will be synced\n",
    "        self.encoder.config = self.config.encoder\n",
    "        self.decoder.config = self.config.decoder\n",
    "\n",
    "        # encoder outputs might need to be projected to different dimension for decoder\n",
    "        if (\n",
    "            self.encoder.config.hidden_size != self.decoder.config.hidden_size\n",
    "            and self.decoder.config.cross_attention_hidden_size is None\n",
    "        ):\n",
    "            self.enc_to_dec_proj = nn.Linear(self.encoder.config.hidden_size, self.decoder.config.hidden_size)\n",
    "\n",
    "        if self.encoder.get_output_embeddings() is not None:\n",
    "            raise ValueError(\n",
    "                f\"The encoder {self.encoder} should not have a LM Head. Please use a model without LM Head\"\n",
    "            )\n",
    "\n",
    "        # tie encoder, decoder weights if config set accordingly\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        # tie encoder & decoder if needed\n",
    "        if self.config.tie_encoder_decoder:\n",
    "            # tie encoder and decoder base model\n",
    "            decoder_base_model_prefix = self.decoder.base_model_prefix\n",
    "            self._tie_encoder_decoder_weights(\n",
    "                self.encoder, self.decoder._modules[decoder_base_model_prefix], self.decoder.base_model_prefix\n",
    "            )\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.encoder.get_input_embeddings()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.decoder.get_output_embeddings()\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        return self.decoder.set_output_embeddings(new_embeddings)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, *args, **kwargs):\n",
    "        # At the moment fast initialization is not supported for composite models\n",
    "        if kwargs.get(\"_fast_init\", False):\n",
    "            logger.warning(\n",
    "                \"Fast initialization is currently not supported for EncoderDecoderModel. \"\n",
    "                \"Falling back to slow initialization...\"\n",
    "            )\n",
    "        kwargs[\"_fast_init\"] = False\n",
    "        return super().from_pretrained(*args, **kwargs)  \n",
    "    \n",
    "      \n",
    "        \n",
    "        \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import setuptokenizer\n",
    "from src.dataset_processor import ContextGenerationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = setuptokenizer('sentence-transformers/all-mpnet-base-v2',special_tokens=[\"#SEP#\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"He #SEP# left\",add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = ContextGenerationDataset(tokenizer,\n",
    "                                   nb_records=1,\n",
    "                                   context_seperator= \"#SEP#\",\n",
    "                                   use_special_token=True,\n",
    "                                   section_boundary=(0.4,0.54),\n",
    "                                   use_random_restrictive=True)\n",
    "dataset.change_data_mode(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> the car was parked near the house of the school #SEP# teacher. there was a cat who was lost. </s>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataset_processor import ContextualGenerationData\n",
    "from pytorch_lightning import seed_everything\n",
    "data = ContextualGenerationData(input=\"\"\"\n",
    "                                The car was parked near the house of the school teacher. There was a cat who was lost.\n",
    "                                \"\"\".replace(\"\\n\",\"\").strip(),output=\"\")\n",
    "\n",
    "batch = dataset.procesTexts(data)\n",
    "tokenizer.batch_decode([batch.input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30528, 768)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "contextual_model = ContextualSentenceTransformerEncoder(model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "                                                        context_delimiter_id= tokenizer.get_vocab()['#SEP#'],\n",
    "                                                        pad_token_id= tokenizer.pad_token_id\n",
    "                                                        ).to(device)\n",
    "contextual_model.model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, torch.Size([1, 23]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_input_ids = batch.input_ids.view(1, -1).to(device)\n",
    "b_input_mask = batch.attention_mask.view(1, -1).to(device)\n",
    "batch.section_point, b_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n"
     ]
    }
   ],
   "source": [
    "enc_output = contextual_model(b_input_ids,b_input_mask,apply_pool=True,return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output.sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/home/nlplab/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f386cbc4cb77425cb3efe297c40f78ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_tokenizer = setuptokenizer('facebook/bart-base',special_tokens=[\"[SEP]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def strip_newline(value):\n",
    "    return ' '.join(value.splitlines())\n",
    "def retrieve_data(data):\n",
    "    pack = []\n",
    "    for dat in data:\n",
    "        zz=bart_tokenizer(dat['document'], return_tensors='pt')['input_ids'].shape\n",
    "        \n",
    "        if zz[-1] < 720:\n",
    "            a= dict(input=strip_newline(dat['document']),output=strip_newline(dat['summary']))\n",
    "            pack.append(a)\n",
    "    data_pack = pd.DataFrame(pack)\n",
    "    data_pack= data_pack.drop_duplicates(subset = [\"output\"],keep=\"last\")\n",
    "    return data_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1140 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train_data = retrieve_data(dataset['train'])\n",
    "dev_data = retrieve_data(dataset['validation'])\n",
    "test_data = retrieve_data(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9163, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('summarisation_data/xsum_train.csv')\n",
    "dev_data.to_csv('summarisation_data/xsum_dev.csv')\n",
    "test_data.to_csv('summarisation_data/xsum_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff= pd.read_csv('summarisation_data/xsum_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset_processor import ContextualGenerationData,read_csv\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = read_csv('summarisation_data/xsum_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextualGenerationData(input='The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed. Repair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water. Trains on the west coast mainline face disruption due to damage at the Lamington Viaduct. Many businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town. First Minister Nicola Sturgeon visited the area to inspect the damage. The waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare. Jeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit. However, she said more preventative work could have been carried out to ensure the retaining wall did not fail. \"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said. \"That may not be true but it is perhaps my perspective over the last few days. \"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\" Meanwhile, a flood alert remains in place across the Borders because of the constant rain. Peebles was badly hit by problems, sparking calls to introduce more defences in the area. Scottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs. The Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand. He said it was important to get the flood protection plan right but backed calls to speed up the process. \"I was quite taken aback by the amount of damage that has been done,\" he said. \"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\" He said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans. Have you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.', output='Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 137])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_tokenizer(dataset['train'][700]['document'], return_tensors='pt')['input_ids'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "473435c5caf2da67d3d84349b3ab99ae605588908510e1f3cdf041055f6c21f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
